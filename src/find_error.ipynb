{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_index_to_title bleach\n",
      "closest_title bleach\n",
      "distance_score 100\n",
      "finding_the_closest_title bleach 100\n",
      "These are the recommendations for similar animes to \u001b[1mbleach\u001b[0m \n",
      "\n",
      "from_title_to_index 800\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8713 features, but NearestNeighbors is expecting 8551 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m create_df(unsupervised_user_based_recommender(\u001b[39m\"\u001b[39;49m\u001b[39mBleach\u001b[39;49m\u001b[39m\"\u001b[39;49m),\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 177\u001b[0m, in \u001b[0;36munsupervised_user_based_recommender\u001b[1;34m(movie_user_likes, n)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m distance_score \u001b[39m==\u001b[39m \u001b[39m100\u001b[39m:\n\u001b[0;32m    176\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThese are the recommendations for similar animes to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[1m\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(closest_title)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m reco(lowertittle,n,pivot_df_try)\n\u001b[0;32m    178\u001b[0m \u001b[39m# When a user makes misspellings    \u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mI guess you misspelled the name\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAre you looking similitudes for the anime named \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[1m\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(closest_title)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mHere are the recommendations:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 149\u001b[0m, in \u001b[0;36mreco\u001b[1;34m(name, n, df)\u001b[0m\n\u001b[0;32m    147\u001b[0m pivot_df \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(processed_data \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpivot_user_based_unsupervised.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    148\u001b[0m indl \u001b[39m=\u001b[39m from_title_to_index(name,df)   \n\u001b[1;32m--> 149\u001b[0m distances, indices \u001b[39m=\u001b[39m model_knn\u001b[39m.\u001b[39;49mkneighbors(pivot_df\u001b[39m.\u001b[39;49miloc[indl,:]\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), n_neighbors \u001b[39m=\u001b[39;49m n\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    150\u001b[0m names_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    151\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\miniconda3\\envs\\stlit\\lib\\site-packages\\sklearn\\neighbors\\_base.py:806\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    804\u001b[0m         X \u001b[39m=\u001b[39m _check_precomputed(X)\n\u001b[0;32m    805\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m         X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    808\u001b[0m n_samples_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_fit_\n\u001b[0;32m    809\u001b[0m \u001b[39mif\u001b[39;00m n_neighbors \u001b[39m>\u001b[39m n_samples_fit:\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\miniconda3\\envs\\stlit\\lib\\site-packages\\sklearn\\base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\miniconda3\\envs\\stlit\\lib\\site-packages\\sklearn\\base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 370\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8713 features, but NearestNeighbors is expecting 8551 features as input."
     ]
    }
   ],
   "source": [
    "create_df(unsupervised_user_based_recommender(\"Bleach\"),\"All\",\"All\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import joblib\n",
    "import pickle\n",
    "from fastparquet import write\n",
    "from fastparquet import write\n",
    "from fuzzywuzzy import fuzz\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from surprise import Dataset, Reader, NormalPredictor, KNNBasic, KNNWithMeans, SVD, accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Utils libraries\n",
    "from utils import utils\n",
    "from utils import cleaning\n",
    "from utils import recommend\n",
    "from utils import testing\n",
    "from utils import training\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")\n",
    "\n",
    "'''\n",
    "Function to return the anime name that mtches de index number\n",
    "'''\n",
    "def from_index_to_title(index,df):\n",
    "    anime = df\n",
    "    print(\"from_index_to_title\",anime[anime.index == index]['name'].values[0])\n",
    "    return anime[anime.index == index]['name'].values[0]\n",
    "\n",
    "\n",
    "'''\n",
    "Function to return the matched index number of the anime name\n",
    "'''\n",
    "def from_title_to_index(title,df):\n",
    "    anime = df\n",
    "    print(\"from_title_to_index\",anime[anime[\"name\"]==title].index.values[0])\n",
    "    return anime[anime[\"name\"]==title].index.values[0]\n",
    "\n",
    "\n",
    "'''\n",
    "Function to find the closest title, It uses Levenshtein Distance to calculate the differences between sequences\n",
    "'''\n",
    "def match_the_score(a,b):\n",
    "   return fuzz.ratio(a,b)\n",
    "\n",
    "\n",
    "'''\n",
    "Function to return the most similar title to the name a user typed\n",
    "'''\n",
    "def finding_the_closest_title(title,df):\n",
    "    anime = df\n",
    "    levenshtein_scores = list(enumerate(anime['name'].apply(match_the_score, b=title))) # Create a list wuth the matchin fuzz.ratio puntuation\n",
    "    sorted_levenshtein_scores = sorted(levenshtein_scores, key=lambda x: x[1], reverse=True) # Sort from higher to lower the scores\n",
    "    closest_title = from_index_to_title(sorted_levenshtein_scores[0][0],anime) # Getting the closest anime name/title\n",
    "    print(\"closest_title\",closest_title)\n",
    "    distance_score = sorted_levenshtein_scores[0][1] # Getting the score\n",
    "    print(\"distance_score\",distance_score)\n",
    "    return closest_title, distance_score\n",
    "\n",
    "'''\n",
    "Function to apply the filters fo the df recommended with animes\n",
    "'''\n",
    "def filtering(df,gen,typ):\n",
    "    \n",
    "    if (gen != \"All\") and (typ != \"All\"):\n",
    "        filtered = df[df['genre'].str.contains(gen, regex=False, case=False, na=False)]\n",
    "        filtered = filtered[filtered['type'].str.contains(typ, regex=False, case=False, na=False)]\n",
    "        return filtered\n",
    "\n",
    "    elif  (gen == \"All\") and (typ != \"All\"):\n",
    "        filtered = df[df['type'].str.contains(typ, regex=False, case=False, na=False)]\n",
    "        return filtered\n",
    "\n",
    "    elif  (typ == \"All\") and (gen != \"All\"):\n",
    "        filtered = df[df['genre'].str.contains(gen, regex=False, case=False, na=False)]\n",
    "        return filtered\n",
    "\n",
    "    elif  (typ == \"All\") and (gen == \"All\"):\n",
    "        return df\n",
    "\n",
    "'''\n",
    "Create a df of the anime matches with the filters selected\n",
    "'''\n",
    "def create_df(names,gen,typ,n=100):\n",
    "    #anime = joblib.load(processed_data + \"/\" + \"_anime_to_compare_with_name.pkl\")\n",
    "    anime = pd.read_csv(processed_data + \"/\" + \"_anime_to_compare_with_name.csv\")# load anime df\n",
    "    final_df = anime[anime['name'].isin(names)]\n",
    "    final_df = final_df.drop(columns=[\"anime_id\", \"members\"])\n",
    "    blankIndex=[''] * len(final_df)\n",
    "    final_df.index=blankIndex\n",
    "    final_df = filtering(final_df,gen,typ)\n",
    "    to_return = final_df.head(n)\n",
    "    if final_df.empty:\n",
    "        sentence = print('WOW!!!! Sorry, there is no matches for the anime and options selected! \\n Try again, you might have mroe luck')\n",
    "        return sentence\n",
    "    else:\n",
    "        return to_return\n",
    "\n",
    "\n",
    "'''\n",
    "Create dict of records with the filters selected - each row becomes a dictionary where key is column name and value is the data in the cell\n",
    "'''\n",
    "def create_dict(names,gen,typ,n=100):\n",
    "    #anime = joblib.load(processed_data  + \"/\" +  \"_anime_to_compare_with_name.pkl\")\n",
    "    anime = pd.read_csv(processed_data + \"/\" + \"_anime_to_compare_with_name.csv\")# load anime df\n",
    "    final_df = anime[anime['name'].isin(names)]\n",
    "    final_df = final_df.drop(columns=[\"anime_id\", \"members\"])\n",
    "    blankIndex=[''] * len(final_df)\n",
    "    final_df.index=blankIndex\n",
    "    final_df = filtering(final_df,gen,typ)\n",
    "    final_df = final_df.head(n)\n",
    "    if final_df.empty:\n",
    "        sentence = print('WOW!!!! Sorry, there is no matches for the anime and options selected! \\n Try again, you might have mroe luck')\n",
    "        return sentence\n",
    "    else:\n",
    "        final_dict = final_df.to_dict('records')\n",
    "\n",
    "        return final_dict\n",
    "\n",
    "'''\n",
    "Return a list with recommendations for the anime \n",
    "'''\n",
    "def reco(name,n,df):\n",
    "    model_knn = joblib.load(saved_models_folder + \"/\" +\"model_matrix_user_based_unsupervised.pkl\")\n",
    "    #shutil.unpack_archive(processed_data + \"/\" + \"pivot_user_based_unsupervised.zip\",processed_data)\n",
    "    #pivot_df = pd.read_csv(processed_data + \"/\" + \"pivot_user_based_unsupervised.zip\")# load anime df\n",
    "    pivot_df = joblib.load(processed_data + \"/\" +\"pivot_user_based_unsupervised.pkl\")\n",
    "    indl = from_title_to_index(name,df)   \n",
    "    distances, indices = model_knn.kneighbors(pivot_df.iloc[indl,:].values.reshape(1, -1), n_neighbors = n+1)\n",
    "    names_list = []\n",
    "    for i in range(1, n+1):\n",
    "        if i == 0:\n",
    "            print('WOW!!!! Sorry, there is no matches for the anime and options selected!\\nTry again, you might have more luck')\n",
    "        else:\n",
    "            names_list.append(pivot_df.index[indices.flatten()[i]])\n",
    "            #print('{0}: {1}'.format(i, pivot_df.index[indices.flatten()[i]]))\n",
    "    \n",
    "    return names_list\n",
    "\n",
    "        \n",
    "\n",
    "'''\n",
    "A function that returns the names of the similar animes\n",
    "for Unsupervised User content based recommendation system\n",
    "'''\n",
    "def unsupervised_user_based_recommender(movie_user_likes,n=200):\n",
    "    #df = joblib.load(processed_data + \"/\" + \"_anime_to_compare_with_name.pkl\")\n",
    "    df = pd.read_csv(processed_data + \"/\" + \"_anime_to_compare_with_name.csv\")# load anime df\n",
    "    lowertittle = movie_user_likes.lower() # Pasamos el titulo a min√∫sculas\n",
    "    #pivot_df_try = joblib.load(processed_data + \"/\" + \"_to_find_index_user_based_unsupervised.pkl\")\n",
    "    pivot_df_try = pd.read_csv(processed_data + \"/\" + \"_to_find_index_user_based_unsupervised.csv\")# load anime df\n",
    "    closest_title, distance_score = finding_the_closest_title(lowertittle,df)\n",
    "    print(\"finding_the_closest_title\",closest_title,distance_score)\n",
    "    # When a user does not make misspellings\n",
    "    if distance_score == 100:\n",
    "        print('These are the recommendations for similar animes to '+'\\033[1m'+str(closest_title)+'\\033[0m'+'','\\n')\n",
    "        return reco(lowertittle,n,pivot_df_try)\n",
    "    # When a user makes misspellings    \n",
    "    else:\n",
    "        print('I guess you misspelled the name\\n\\nAre you looking similitudes for the anime named '+'\\033[1m'+str(closest_title)+'\\033[0m'+'?','\\n' + '\\nHere are the recommendations:\\n')\n",
    "        return reco(closest_title,n,pivot_df_try)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8120e94be8c887b90a4a4cecf0d7be35c08afb61c8d77a193b8a384ad1a0b7e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
