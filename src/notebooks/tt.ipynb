{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os # allows access to OS-dependent functionalities\n",
    "import sys # to manipulate different parts of the Python runtime environment\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import joblib # set of tools to provide lightweight pipelining in Python\n",
    "\n",
    "# Surprise libraries\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "from surprise import SVD, SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Utils libraries\n",
    "from cleaning import *\n",
    "from recommend import *\n",
    "from testing import *\n",
    "from training import *\n",
    "\n",
    "#Preparing folder variables\n",
    "\n",
    "main_folder = os.path.abspath(os.path.join(os.pardir))\n",
    "data_folder = (main_folder + \"/\" +\"data\")\n",
    "saved_models_folder = (data_folder + \"/\" + \"saved_models\")\n",
    "raw_data = (data_folder + \"/\" + \"_raw\")\n",
    "processed_data = (data_folder + \"/\" + \"processed\")\n",
    "baseline_data = (saved_models_folder + \"/\" + \"baseline\")\n",
    "test_models = (saved_models_folder + \"/\" + \"test_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_different_models():\n",
    "    '''\n",
    "    The code reads two CSV files (anime.csv and rating.csv.zip) and loads them into dataframes. \n",
    "    Then it creates a subset of the rating dataframe containing only rows where the rating is \n",
    "    greater than 0 and removes the index column. Next, it samples a subset of the data with \n",
    "    a specified size, grouped by the rating column.\n",
    "    '''\n",
    "    # Load 'anime.csv' file into a pandas DataFrame object called 'anime'\n",
    "    anime = pd.read_csv(raw_data + \"/\" + \"anime.csv\")\n",
    "\n",
    "    # Load 'rating.csv.zip' file into a pandas DataFrame object called 'rating'\n",
    "    rating = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "    # Create a new DataFrame 'anime_mapping' that is a copy of the 'anime' DataFrame and remove the 'episodes', 'members', and 'rating' columns\n",
    "    anime_mapping = anime.copy()\n",
    "    anime_mapping.drop(['episodes','members','rating'],axis=1, inplace=True)\n",
    "\n",
    "    # Filter out all ratings less than or equal to 0 and reset the index of the DataFrame\n",
    "    ratingdf = rating[rating.rating>0]\n",
    "    ratingdf = ratingdf.reset_index()\n",
    "\n",
    "    # Drop the 'index' column and update the DataFrame in-place\n",
    "    ratingdf.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Get the shape of the DataFrame 'ratingdf'\n",
    "    ratingdf.shape\n",
    "\n",
    "    # Set the size to 1,000,000 and sample from the 'ratingdf' DataFrame based on the proportion of ratings for each score\n",
    "    size = 1000000\n",
    "\n",
    "    # This will make sure that the sampled data has roughly the same proportion of ratings for each score as the original data\n",
    "    ratingdf_sample = ratingdf.groupby(\"rating\", group_keys=False).apply(lambda x: x.sample(int(np.rint(size*len(x)/len(ratingdf))))).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Create a new 'Reader' object with the rating scale set to a range between 1 and 10\n",
    "    reader = Reader(rating_scale=(1,10))\n",
    "\n",
    "    # Load the sampled data into a 'Dataset' object using the 'load_from_df' method and the 'reader' object\n",
    "    data = Dataset.load_from_df(ratingdf_sample[['user_id', 'anime_id', 'rating']], reader)\n",
    "\n",
    "    # Saving the table to pickle\n",
    "    joblib.dump(data,processed_data + \"/\" + \"data_reader_for_different_models.pkl\")\n",
    "\n",
    "    return data\n",
    "data = prepare_for_different_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SlopeOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SlopeOne is a fast and simple algorithm for predicting user ratings in collaborative filtering. It computes the average difference in rating (the slope) between pairs of items, and then uses these slopes to predict the rating of a target item for a given user based on their previous ratings for other items. The algorithm is well-suited to large-scale tasks and has been shown to be effective in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SlopeOne does not have any hyperparameters to tune in Surprise library. Therefore, we will perfom the training and get the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(predictions):\n",
    "    \"\"\"\n",
    "    Compute the Mean Absolute Percentage Error (MAPE) for a set of predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: list of Prediction objects returned by the test method of an algorithm\n",
    "        \n",
    "    Returns:\n",
    "        The MAPE score\n",
    "    \"\"\"\n",
    "    actual_ratings = np.array([pred.r_ui for pred in predictions])\n",
    "    predicted_ratings = np.array([pred.est for pred in predictions])\n",
    "    return np.mean(np.abs(actual_ratings - predicted_ratings) / actual_ratings) * 100\n",
    "\n",
    "def r2(predictions):\n",
    "    \"\"\"\n",
    "    Compute the R-squared (R2) score for a set of predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: list of Prediction objects returned by the test method of an algorithm\n",
    "        \n",
    "    Returns:\n",
    "        The R2 score\n",
    "    \"\"\"\n",
    "    actual_ratings = np.array([pred.r_ui for pred in predictions])\n",
    "    predicted_ratings = np.array([pred.est for pred in predictions])\n",
    "    mean_rating = np.mean(actual_ratings)\n",
    "    ss_tot = np.sum((actual_ratings - mean_rating) ** 2)\n",
    "    ss_res = np.sum((actual_ratings - predicted_ratings) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.2746\n",
      "MAE:  0.9617\n",
      "MSE: 1.6245\n",
      "RMSE: 1.2745619937093307\n",
      "MSE: 1.624508275808304\n",
      "MAE: 0.9617043084174556\n",
      "mape_score: 15.125942719482898\n",
      "r2_score: 0.3454769208138584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['d:\\\\Github\\\\Anime_recommendation_system\\\\src/data/saved_models/SlopeOne_model.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splits the data into training and testing sets with a 80:20 ratio\n",
    "trainset, testset = train_test_split(data, test_size=0.2)       \n",
    "\n",
    "# Create a SlopeOne model and fit it to the training set\n",
    "model = SlopeOne()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Use the model to make predictions on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Calculates the RMSE and MAE for the predictions\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions) \n",
    "mse = accuracy.mse(predictions)\n",
    "mape_score = mape(predictions)\n",
    "r2_score = r2(predictions)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MSE:\", mse)  \n",
    "print(\"MAE:\", mae)  \n",
    "print(\"mape_score:\", mape_score)  \n",
    "print(\"r2_score:\", r2_score)    \n",
    "\n",
    "# Saves the trained model as a pickle file using joblib\n",
    "joblib.dump(model,saved_models_folder + \"/\" + \"SlopeOne_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Selected models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected the 4 models with best results in the baseline. Now lets evaluate them doing GridSearchCV and training them to get the results.\n",
    "\n",
    "- svd = SVD()\n",
    "- svdp = SVDpp()\n",
    "- baseonly = BaselineOnly()\n",
    "- coclus = CoClustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chrisitan\\miniconda3\\envs\\stlit\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os # allows access to OS-dependent functionalities\n",
    "import sys # to manipulate different parts of the Python runtime environment\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Utils libraries\n",
    "from cleaning import *\n",
    "from recommend import *\n",
    "from testing import *\n",
    "from training import *\n",
    "\n",
    "#Preparing folder variables\n",
    "\n",
    "main_folder = os.path.abspath(os.path.join(os.pardir))\n",
    "data_folder = (main_folder + \"/\" +\"data\")\n",
    "saved_models_folder = (data_folder + \"/\" + \"saved_models\")\n",
    "raw_data = (data_folder + \"/\" + \"_raw\")\n",
    "processed_data = (data_folder + \"/\" + \"processed\")\n",
    "baseline_data = (saved_models_folder + \"/\" + \"baseline\")\n",
    "test_models = (saved_models_folder + \"/\" + \"test_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data, reducing the sample to 1million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_different_models():\n",
    "    '''\n",
    "    The code reads two CSV files (anime.csv and rating.csv.zip) and loads them into dataframes. \n",
    "    Then it creates a subset of the rating dataframe containing only rows where the rating is \n",
    "    greater than 0 and removes the index column. Next, it samples a subset of the data with \n",
    "    a specified size, grouped by the rating column.\n",
    "    '''\n",
    "    # Load 'anime.csv' file into a pandas DataFrame object called 'anime'\n",
    "    anime = pd.read_csv(raw_data + \"/\" + \"anime.csv\")\n",
    "\n",
    "    # Load 'rating.csv.zip' file into a pandas DataFrame object called 'rating'\n",
    "    rating = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "    # Create a new DataFrame 'anime_mapping' that is a copy of the 'anime' DataFrame and remove the 'episodes', 'members', and 'rating' columns\n",
    "    anime_mapping = anime.copy()\n",
    "    anime_mapping.drop(['episodes','members','rating'],axis=1, inplace=True)\n",
    "\n",
    "    # Filter out all ratings less than or equal to 0 and reset the index of the DataFrame\n",
    "    ratingdf = rating[rating.rating>0]\n",
    "    ratingdf = ratingdf.reset_index()\n",
    "\n",
    "    # Drop the 'index' column and update the DataFrame in-place\n",
    "    ratingdf.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Get the shape of the DataFrame 'ratingdf'\n",
    "    ratingdf.shape\n",
    "\n",
    "    # Set the size to 1,000,000 and sample from the 'ratingdf' DataFrame based on the proportion of ratings for each score\n",
    "    size = 1000000\n",
    "\n",
    "    # This will make sure that the sampled data has roughly the same proportion of ratings for each score as the original data\n",
    "    ratingdf_sample = ratingdf.groupby(\"rating\", group_keys=False).apply(lambda x: x.sample(int(np.rint(size*len(x)/len(ratingdf))))).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Create a new 'Reader' object with the rating scale set to a range between 1 and 10\n",
    "    reader = Reader(rating_scale=(1,10))\n",
    "\n",
    "    # Load the sampled data into a 'Dataset' object using the 'load_from_df' method and the 'reader' object\n",
    "    data = Dataset.load_from_df(ratingdf_sample[['user_id', 'anime_id', 'rating']], reader)\n",
    "\n",
    "    # Saving the table to pickle\n",
    "    #joblib.dump(data,processed_data + \"/\" + \"data_reader_for_different_models.pkl\")\n",
    "\n",
    "    return data\n",
    "data = prepare_for_different_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular Value Decomposition (SVD) is a matrix factorization technique used in recommendation systems to reduce the dimensionality of a user-item matrix and identify latent factors that drive user-item interactions. In essence, SVD represents the original matrix as the product of three matrices: a user matrix, a singular value matrix, and an item matrix. The resulting factors can be used to make personalized recommendations by predicting a user's preference for an item based on their past behavior and the behavior of other similar users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the important hyperparameters for the SVD algorithm in the Surprise library:\n",
    "\n",
    "- **n_factors:** The number of factors to use in the matrix factorization. This controls the number of latent features to be learned from the data.\n",
    "- **n_epochs:** The number of epochs (iterations) to run the matrix factorization algorithm.\n",
    "- **biased:** A boolean indicating whether or not to use biases in the model. Biases represent the average rating for each user and item, and can help improve the accuracy of the predictions.\n",
    "- **lr_all:** The learning rate for all parameters in the model. This controls the step size for each iteration of the optimization algorithm.\n",
    "- **reg_all:** The regularization strength for all parameters in the model. This helps prevent overfitting by adding a penalty term to the optimization objective that encourages the model to have smaller parameter values.\n",
    "- **init_mean:** The mean of the Gaussian distribution used to initialize the factor matrices. By default, this is set to 0.\n",
    "- **init_std_dev:** The standard deviation of the Gaussian distribution used to initialize the factor matrices. By default, this is set to 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of find_best_svd:\n",
    "- Define parameter grid for grid search\n",
    "- Create GridSearchCV object with SVD algorithm\n",
    "- Fit GridSearchCV object to data\n",
    "- Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "- Save model with best parameters\n",
    "- Save best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score: 1.1183239072148927\n",
      "Best MAE score: 0.8364221843283999\n",
      "Best parameters for RMSE: {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.05}\n",
      "Best parameters for MAE: {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.model_selection.search.GridSearchCV at 0x1bd2c815b80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_best_svd(data):\n",
    "    '''\n",
    "    defines a parameter grid for hyperparameter tuning in a collaborative filtering algorithm.\n",
    "    Then create a GridSearchCV object with the SVD algorithm and a parameter grid consisting \n",
    "    of a range of hyperparameters. The GridSearchCV function then performs a grid search on \n",
    "    yhe parameter grid to find the best combination of hyperparameters that minimizes the \n",
    "    RMSE and MAE scores. The best RMSE and MAE scores and the corresponding parameters \n",
    "    are printed out.\n",
    "    '''\n",
    "    from surprise import SVD\n",
    "    from surprise.model_selection import GridSearchCV\n",
    "    data = data\n",
    "\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {'n_factors': [50, 100, 150, 300], \n",
    "                'n_epochs': [20, 30, 40, 50], \n",
    "                'lr_all': [0.002, 0.005, 0.01, 0.1],\n",
    "                'reg_all': [0.02, 0.05, 0.1]}\n",
    "\n",
    "    # Create GridSearchCV object with SVD algorithmr\n",
    "    gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae','mse'], cv=5)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "    \n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"SVD_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"SVD_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    return gs\n",
    "best_params = find_best_svd(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVDpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD++ is an extension of Singular Value Decomposition (SVD) that takes into account implicit feedback in recommendation systems. In addition to the user-item matrix used in SVD, SVD++ also considers a matrix of implicit feedback such as user interactions with items, item attributes, and user preferences. This additional matrix helps capture the influence of user and item biases on the recommendation process, resulting in more accurate and personalized recommendations. SVD++ also includes a regularization term to avoid overfitting and improve generalization. Overall, SVD++ is a more advanced and sophisticated technique for recommendation systems than SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVDpp algorithm in the Surprise library has the following hyperparameters:\n",
    "\n",
    "- **n_factors:** the number of latent factors. Default is 20.\n",
    "- **n_epochs:** the number of iterations for the optimization algorithms. Default is 20.\n",
    "- **lr_all:** the learning rate for all parameters. Default is 0.005.\n",
    "- **reg_all:** the regularization parameter for all parameters. Default is 0.02.\n",
    "- **init_mean:** the mean of the normal distribution for factor vectors initialization. Default is 0.\n",
    "- **init_std_dev:** the standard deviation of the normal distribution for factor vectors initialization. Default is 0.1.\n",
    "- **verbose:** whether to print details during the optimization process. Default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_svdpp(data):\n",
    "    from surprise import SVDpp\n",
    "    from surprise.model_selection import GridSearchCV\n",
    "    import joblib\n",
    "\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {'n_factors': [50, 100, 150], \n",
    "                  'n_epochs': [20, 30, 40], \n",
    "                  'lr_all': [0.002, 0.005, 0.01],\n",
    "                  'reg_all': [0.02, 0.05, 0.1]\n",
    "                  }\n",
    "\n",
    "    # Create GridSearchCV object with SVDpp algorithm\n",
    "    gs = GridSearchCV(SVDpp, param_grid, measures=['rmse', 'mae','mse'], cv=5)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"SVDpp_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"SVDpp_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    return gs\n",
    "find_best_svdpp(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaselineOnly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaselineOnly is a simple but effective collaborative filtering algorithm used in recommendation systems. It predicts a user's rating for an item by taking into account the overall average rating of all items, the average rating of the user, and the average rating of the item. These three values are used as baseline estimates, and the difference between the actual rating and the baseline estimate is used as the prediction error. BaselineOnly then learns user and item biases that can improve the accuracy of the baseline estimates. The algorithm is simple and computationally efficient, making it a popular choice for recommendation systems with large datasets. However, it may not capture more complex relationships between users and items compared to more advanced techniques such as matrix factorization.BaselineOnly is a simple but effective collaborative filtering algorithm used in recommendation systems. It predicts a user's rating for an item by taking into account the overall average rating of all items, the average rating of the user, and the average rating of the item. These three values are used as baseline estimates, and the difference between the actual rating and the baseline estimate is used as the prediction error. BaselineOnly then learns user and item biases that can improve the accuracy of the baseline estimates. The algorithm is simple and computationally efficient, making it a popular choice for recommendation systems with large datasets. However, it may not capture more complex relationships between users and items compared to more advanced techniques such as matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaselineOnly algorithm in the Surprise library has the following hyperparameters:\n",
    "- **bsl_options:** a dictionary containing the following options for the baseline estimates:\n",
    "    - **method:** the method used to compute the baseline estimates. Possible values are: als, sgd. Default is als.\n",
    "    - **n_epochs:** the number of iterations for the optimization algorithms. Default is 20.\n",
    "    - **reg_u:** the regularization parameter for users. Default is 15.\n",
    "    - **reg_i:** the regularization parameter for items. Default is 10.\n",
    "    - **verbose**: whether to print details during the optimization process. Default is False.\n",
    "- **biased:** whether to include the baseline estimate in the prediction. Default is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "def find_best_BaselineOnly(data):\n",
    "    from surprise import BaselineOnly\n",
    "\n",
    "    from surprise.model_selection import GridSearchCV\n",
    "    import joblib\n",
    "\n",
    "    # Parameters docs and value ranges:\n",
    "    # http://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baseline-estimates-configuration\n",
    "    # http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf\n",
    " \n",
    "     # Define parameter grid for grid search\n",
    "\n",
    "    param_grid = {'bsl_options': {'method': ['sgd', 'als'],\n",
    "                                'reg': [0.02, 0.05, 0.1],\n",
    "                                'learning_rate': [0.001, 0.005, 0.01],\n",
    "                                'n_epochs': [5, 10, 15],\n",
    "                                'verbose': [True]},\n",
    "                'verbose': [True]}\n",
    "\n",
    "    # Create GridSearchCV object with SVDpp algorithm\n",
    "    gs = GridSearchCV(BaselineOnly, param_grid, measures=['rmse', 'mae','mse'], cv=5, n_jobs=1)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"BaselineOnly_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"BaselineOnly_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    \n",
    "    return gs\n",
    "\n",
    "find_best_BaselineOnly(data)\n",
    "\n",
    "#### Saving the output of this cell to a file\n",
    "# Check if the file exists\n",
    "if os.path.exists(os.path.join(test_models + \"/\" + \"capture_BaselineOnly.txt\")):\n",
    "\n",
    "    # If the file exists, open it in \"a\" mode to append to the end\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_BaselineOnly.txt\"), \"a\") as f:\n",
    "        \n",
    "        # Write the captured output to the end of the file\n",
    "        f.write(cap.stdout)       \n",
    "else:\n",
    "    # If the file does not exist, create it and open it in \"w\" mode to write to it\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_BaselineOnly.txt\"), \"w\") as f:\n",
    "\n",
    "        # Write the captured output to the file\n",
    "        f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best RMSE score: 1.5566270718662043\n",
    "- Best MSE score: 2.426553220392917\n",
    "- Best MAE score: 1.2098539052864254\n",
    "- Best parameters for RMSE: {'bsl_options': {'method': 'sgd', 'reg': 0.02, 'learning_rate': 0.01, 'n_epochs': 15, 'verbose': True}, 'verbose': True}\n",
    "- Best parameters for MAE: {'bsl_options': {'method': 'sgd', 'reg': 0.02, 'learning_rate': 0.01, 'n_epochs': 15, 'verbose': True}, 'verbose': True}\n",
    "- Best parameters for MSE: {'bsl_options': {'method': 'sgd', 'reg': 0.02, 'learning_rate': 0.01, 'n_epochs': 15, 'verbose': True}, 'verbose': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoClustering is a collaborative filtering algorithm for recommendation systems that groups users and items into clusters and then estimates the ratings based on the interactions between these clusters. The algorithm tries to find a block diagonal structure in the user-item matrix, where users and items are clustered together. This approach can be more effective than traditional matrix factorization techniques in cases where users or items have similar tastes or properties. The CoClustering algorithm is available in the Surprise library, which is a popular Python library for building and evaluating recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief explanation of these hyperparameters:\n",
    "\n",
    "- **n_cltr_u** and **n_cltr_i:** the number of user and item clusters, respectively. This is a crucial hyperparameter for CoClustering, as it determines the level of granularity of the clustering. Higher values will generally result in better performance but may also increase training time.\n",
    "- **n_epochs:** the number of epochs or iterations to run the algorithm for. This is also an important hyperparameter, as it determines the number of times the algorithm will iterate over the data. More iterations can lead to better performance but may also increase training time.\n",
    "- **verbose:** a boolean flag indicating whether or not to print progress messages during training.\n",
    "- **random_state:** an integer value representing the random seed used to initialize the algorithm's parameters. This can be useful for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise libraries\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "from surprise import CoClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "def find_best_coclustering(data):\n",
    "\n",
    "\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {'n_cltr_u': [3, 5, 10], \n",
    "                'n_cltr_i': [3, 5, 10], \n",
    "                'n_epochs': [20, 30, 40], \n",
    "                'verbose': [True, False], \n",
    "                'random_state': [42, 123]\n",
    "                }\n",
    "\n",
    "    # Create GridSearchCV object with SVDpp algorithm\n",
    "    gs = GridSearchCV(CoClustering, param_grid, measures=['rmse', 'mae','mse'], cv=5)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"CoClustering_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"CoClustering_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    return gs\n",
    "find_best_coclustering(data)\n",
    "\n",
    "\n",
    "#### Saving the output of this cell to a file\n",
    "# Check if the file exists\n",
    "if os.path.exists(os.path.join(test_models + \"/\" + \"capture_CoClustering.txt\")):\n",
    "\n",
    "    # If the file exists, open it in \"a\" mode to append to the end\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_CoClustering.txt\"), \"a\") as f:\n",
    "        \n",
    "        # Write the captured output to the end of the file\n",
    "        f.write(cap.stdout)       \n",
    "else:\n",
    "    # If the file does not exist, create it and open it in \"w\" mode to write to it\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_CoClustering.txt\"), \"w\") as f:\n",
    "\n",
    "        # Write the captured output to the file\n",
    "        f.write(cap.stdout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cecd28889de614796b096279bba34faf450ea72cc4d265581beb94f7bfeace2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
