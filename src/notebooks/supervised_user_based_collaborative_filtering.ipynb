{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised user based collaborative filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os # allows access to OS-dependent functionalities\n",
    "import re #  regular expression matching operations similar to those found in Perl\n",
    "import sys # to manipulate different parts of the Python runtime environment\n",
    "import warnings # is used to display the message Warning\n",
    "import pickle # serializing and deserializing a Python object structure.\n",
    "\n",
    "# Third party libraries\n",
    "from fastparquet import write # parquet format, aiming integrate into python-based big data work-flows\n",
    "from fuzzywuzzy import fuzz # used for string matching\n",
    "\n",
    "import numpy as np # functions for working in domain of linear algebra, fourier transform, matrices and arrays\n",
    "import pandas as pd # data analysis and manipulation tool\n",
    "import joblib # set of tools to provide lightweight pipelining in Python\n",
    "import glob\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt # collection of functions that make matplotlib work like MATLAB.\n",
    "\n",
    "# Surprise libraries\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "from surprise import SVD, SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering\n",
    "\n",
    "# pip install git+https://github.com/NicolasHug/surprise.git\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Utils libraries\n",
    "from cleaning import *\n",
    "from recommend import *\n",
    "from testing import *\n",
    "from training import *\n",
    "\n",
    "#Preparing folder variables\n",
    "\n",
    "main_folder = os.path.abspath(os.path.join(os.pardir))\n",
    "data_folder = (main_folder + \"/\" +\"data\")\n",
    "saved_models_folder = (data_folder + \"/\" + \"saved_models\")\n",
    "raw_data = (data_folder + \"/\" + \"_raw\")\n",
    "processed_data = (data_folder + \"/\" + \"processed\")\n",
    "baseline_data = (saved_models_folder + \"/\" + \"baseline\")\n",
    "test_models = (saved_models_folder + \"/\" + \"test_models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file called \"anime.csv\" from a directory called raw_data and returns the contents as a Pandas DataFrame\n",
    "anime = pd.read_csv(raw_data + \"/\" + \"anime.csv\") \n",
    "\n",
    "# CSV file called \"rating.csv.zip\" from a directory called raw_data and returns the contents as a Pandas DataFrame\n",
    "rating = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  anime_id  rating\n",
       "0        1        20      -1\n",
       "1        1        24      -1\n",
       "2        1        79      -1\n",
       "3        1       226      -1\n",
       "4        1       241      -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7813737, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.813737e+06</td>\n",
       "      <td>7.813737e+06</td>\n",
       "      <td>7.813737e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.672796e+04</td>\n",
       "      <td>8.909072e+03</td>\n",
       "      <td>6.144030e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.099795e+04</td>\n",
       "      <td>8.883950e+03</td>\n",
       "      <td>3.727800e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.897400e+04</td>\n",
       "      <td>1.240000e+03</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.679100e+04</td>\n",
       "      <td>6.213000e+03</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.475700e+04</td>\n",
       "      <td>1.409300e+04</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.351600e+04</td>\n",
       "      <td>3.451900e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id      anime_id        rating\n",
       "count  7.813737e+06  7.813737e+06  7.813737e+06\n",
       "mean   3.672796e+04  8.909072e+03  6.144030e+00\n",
       "std    2.099795e+04  8.883950e+03  3.727800e+00\n",
       "min    1.000000e+00  1.000000e+00 -1.000000e+00\n",
       "25%    1.897400e+04  1.240000e+03  6.000000e+00\n",
       "50%    3.679100e+04  6.213000e+03  7.000000e+00\n",
       "75%    5.475700e+04  1.409300e+04  9.000000e+00\n",
       "max    7.351600e+04  3.451900e+04  1.000000e+01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(supervised_rating_cleaning.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of supervised_rating_cleaning:\n",
    "- selects only those rows from the 'rating' DataFrame where the value of the 'rating' column is greater than 0. The resulting DataFrame is assigned to the variable 'ratingdf'.\n",
    "- resets the index of the 'ratingdf' DataFrame. This means that the current index is replaced with a sequential index starting from 0, and a new column called 'index' is added to the DataFrame to store the old index values.\n",
    "- drops the 'index' column from the 'ratingdf' DataFrame. The 'axis=1' argument specifies that the column should be dropped, and 'inplace=True' means that the changes should be made to the DataFrame in place (i.e., the DataFrame is modified directly rather than creating a copy).\n",
    "- returns the modified 'ratingdf' DataFrame as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "ratingdf = supervised_rating_cleaning(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6337241, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingdf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we are goin to prepare de data to be used in the model baseline.\n",
    "\n",
    "To do a baseline we will use a smaller dataset and then use the whole dataset in the selected mode. Because using a smaller dataset for prototyping and testing can be a good way to quickly iterate and experiment with different algorithms and hyperparameters before scaling up to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_different_models():\n",
    "    '''\n",
    "    The code reads two CSV files (anime.csv and rating.csv.zip) and loads them into dataframes. \n",
    "    Then it creates a subset of the rating dataframe containing only rows where the rating is \n",
    "    greater than 0 and removes the index column. Next, it samples a subset of the data with \n",
    "    a specified size, grouped by the rating column.\n",
    "    '''\n",
    "    # Load 'anime.csv' file into a pandas DataFrame object called 'anime'\n",
    "    anime = pd.read_csv(raw_data + \"/\" + \"anime.csv\")\n",
    "\n",
    "    # Load 'rating.csv.zip' file into a pandas DataFrame object called 'rating'\n",
    "    rating = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "    # Create a new DataFrame 'anime_mapping' that is a copy of the 'anime' DataFrame and remove the 'episodes', 'members', and 'rating' columns\n",
    "    anime_mapping = anime.copy()\n",
    "    anime_mapping.drop(['episodes','members','rating'],axis=1, inplace=True)\n",
    "\n",
    "    # Filter out all ratings less than or equal to 0 and reset the index of the DataFrame\n",
    "    ratingdf = rating[rating.rating>0]\n",
    "    ratingdf = ratingdf.reset_index()\n",
    "\n",
    "    # Drop the 'index' column and update the DataFrame in-place\n",
    "    ratingdf.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Get the shape of the DataFrame 'ratingdf'\n",
    "    ratingdf.shape\n",
    "\n",
    "    # Set the size to 100,000 and sample from the 'ratingdf' DataFrame based on the proportion of ratings for each score\n",
    "    size = 100000\n",
    "\n",
    "    # This will make sure that the sampled data has roughly the same proportion of ratings for each score as the original data\n",
    "    ratingdf_sample = ratingdf.groupby(\"rating\", group_keys=False).apply(lambda x: x.sample(int(np.rint(size*len(x)/len(ratingdf))))).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Create a new 'Reader' object with the rating scale set to a range between 1 and 10\n",
    "    reader = Reader(rating_scale=(1,10))\n",
    "\n",
    "    # Load the sampled data into a 'Dataset' object using the 'load_from_df' method and the 'reader' object\n",
    "    data = Dataset.load_from_df(ratingdf_sample[['user_id', 'anime_id', 'rating']], reader)\n",
    "\n",
    "    # Saving the table to pickle\n",
    "    joblib.dump(data,processed_data + \"/\" + \"data_reader_for_different_models.pkl\")\n",
    "\n",
    "    return data\n",
    "data = prepare_for_different_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function that performs cross-validation for several collaborative filtering algorithms using the Surprise library and returns the results in a pandas DataFrame. The function takes a Surprise dataset object as input and outputs two DataFrames, one with the results for each individual algorithm and another with the results for all algorithms.\n",
    "\n",
    "The algorithms used in this function are SVD, SVD++, SlopeOne, NMF, NormalPredictor, BaselineOnly, and CoClustering. For each algorithm, the function performs 5-fold cross-validation and computes the RMSE, MSE, MAE, and FCP metrics. The results are then stored in a DataFrame and saved to a file.\n",
    "\n",
    "Note that the function only runs the SVD algorithm by default. To run other algorithms, you need to uncomment the relevant lines in the for loop. Also, the function assumes that the saved_models_folder variable has been defined elsewhere in the code.\n",
    "\n",
    "Overall, this function provides a basic implementation of collaborative filtering algorithms using the Surprise library and can be used as a starting point for building more sophisticated recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import KFold\n",
    "'''\n",
    "Function that runs several collaborative filtering algorithms on an input dataset using cross-validation \n",
    "and computes several evaluation metrics. The function loops through a list of algorithms, \n",
    "runs cross-validation with each algorithm, computes the mean evaluation results across all folds, \n",
    "and appends the results to an overall list of results. It then saves the evaluation results \n",
    "of each algorithm in a Parquet file and saves the overall evaluation results of all algorithms \n",
    "in another Parquet file. The function returns the overall evaluation results.\n",
    "'''\n",
    "def baseline_all(data):\n",
    "\n",
    "    # instantiate the collaborative filtering algorithms we want to evaluate\n",
    "    svd = SVD()\n",
    "    svdp = SVDpp()\n",
    "    slpo = SlopeOne()\n",
    "    nm  = NMF()\n",
    "    nmlp = NormalPredictor()\n",
    "    #knnbase = KNNBaseline()\n",
    "    #knnb = KNNBasic()\n",
    "    #knnmean = KNNWithMeans()\n",
    "    #knnzs = KNNWithZScore()\n",
    "    baseonly = BaselineOnly()\n",
    "    coclus = CoClustering()\n",
    "\n",
    "    # loop through each algorithm and evaluate it using 5-fold cross-validation\n",
    "    for algorithm in [svd,svdp,slpo,nm,nmlp,baseonly,coclus]:\n",
    "\n",
    "        # create an empty list to hold the benchmark results for this algorithm\n",
    "        benchmark_inndividual = []\n",
    "        \n",
    "        # print a message to indicate which algorithm is being evaluated\n",
    "        print(algorithm,\"started\")\n",
    "\n",
    "        folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # run 5-fold cross-validation and compute RMSE, MSE, MAE, and FCP metrics\n",
    "        results = cross_validate(algorithm, data, measures=['RMSE','MSE','MAE','FCP'], cv=folds, verbose=True)\n",
    "\n",
    "        # print a message to indicate that the algorithm has finished evaluating\n",
    "        print(algorithm,\"finished\")\n",
    "\n",
    "        # calculate the mean of each metric over the 5 folds and store the results in a DataFrame\n",
    "        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "\n",
    "        # extract the name of the algorithm from the object and append it to the DataFrame\n",
    "        name = str(algorithm).split(' ')[0].split('.')[-1]\n",
    "        tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "\n",
    "        # add the results for this algorithm to the list of individual benchmark results\n",
    "        benchmark_inndividual.append(tmp)\n",
    "\n",
    "        # convert the list of results for this algorithm to a DataFrame and save it to a file\n",
    "        dfscores_individual = pd.DataFrame(benchmark_inndividual).set_index('Algorithm').sort_values('test_rmse')\n",
    "        write(baseline_data + \"/\" + name + \"_results.parq\", dfscores_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_all(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging results from Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_fcp</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>1.205758</td>\n",
       "      <td>1.453853</td>\n",
       "      <td>0.916848</td>\n",
       "      <td>0.707061</td>\n",
       "      <td>31.520678</td>\n",
       "      <td>45.932104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoClustering</th>\n",
       "      <td>1.211017</td>\n",
       "      <td>1.466567</td>\n",
       "      <td>0.915281</td>\n",
       "      <td>0.711172</td>\n",
       "      <td>358.642635</td>\n",
       "      <td>53.776164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>1.498532</td>\n",
       "      <td>2.245621</td>\n",
       "      <td>1.156121</td>\n",
       "      <td>0.535850</td>\n",
       "      <td>32.409142</td>\n",
       "      <td>1.718590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBasic</th>\n",
       "      <td>1.643655</td>\n",
       "      <td>2.701632</td>\n",
       "      <td>1.275451</td>\n",
       "      <td>0.462612</td>\n",
       "      <td>35.482900</td>\n",
       "      <td>1.732444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithMeans</th>\n",
       "      <td>1.651630</td>\n",
       "      <td>2.727919</td>\n",
       "      <td>1.258240</td>\n",
       "      <td>0.466116</td>\n",
       "      <td>31.944902</td>\n",
       "      <td>1.796209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>1.666752</td>\n",
       "      <td>2.778087</td>\n",
       "      <td>1.267275</td>\n",
       "      <td>0.468789</td>\n",
       "      <td>36.883558</td>\n",
       "      <td>2.038430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF</th>\n",
       "      <td>2.230265</td>\n",
       "      <td>4.974104</td>\n",
       "      <td>1.978937</td>\n",
       "      <td>0.693325</td>\n",
       "      <td>227.350313</td>\n",
       "      <td>54.632843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NormalPredictor</th>\n",
       "      <td>2.148245</td>\n",
       "      <td>4.614956</td>\n",
       "      <td>1.704465</td>\n",
       "      <td>0.497267</td>\n",
       "      <td>17.417936</td>\n",
       "      <td>56.607932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SlopeOne</th>\n",
       "      <td>1.196530</td>\n",
       "      <td>1.431685</td>\n",
       "      <td>0.903780</td>\n",
       "      <td>0.711841</td>\n",
       "      <td>184.353579</td>\n",
       "      <td>479.512936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVDpp</th>\n",
       "      <td>1.191880</td>\n",
       "      <td>1.420580</td>\n",
       "      <td>0.886783</td>\n",
       "      <td>0.730786</td>\n",
       "      <td>4176.325600</td>\n",
       "      <td>857.291792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>1.132956</td>\n",
       "      <td>1.283591</td>\n",
       "      <td>0.844734</td>\n",
       "      <td>0.737092</td>\n",
       "      <td>129.236504</td>\n",
       "      <td>45.101050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_rmse  test_mse  test_mae  test_fcp     fit_time  \\\n",
       "Algorithm                                                               \n",
       "BaselineOnly      1.205758  1.453853  0.916848  0.707061    31.520678   \n",
       "CoClustering      1.211017  1.466567  0.915281  0.711172   358.642635   \n",
       "KNNBaseline       1.498532  2.245621  1.156121  0.535850    32.409142   \n",
       "KNNBasic          1.643655  2.701632  1.275451  0.462612    35.482900   \n",
       "KNNWithMeans      1.651630  2.727919  1.258240  0.466116    31.944902   \n",
       "KNNWithZScore     1.666752  2.778087  1.267275  0.468789    36.883558   \n",
       "NMF               2.230265  4.974104  1.978937  0.693325   227.350313   \n",
       "NormalPredictor   2.148245  4.614956  1.704465  0.497267    17.417936   \n",
       "SlopeOne          1.196530  1.431685  0.903780  0.711841   184.353579   \n",
       "SVDpp             1.191880  1.420580  0.886783  0.730786  4176.325600   \n",
       "SVD               1.132956  1.283591  0.844734  0.737092   129.236504   \n",
       "\n",
       "                  test_time  \n",
       "Algorithm                    \n",
       "BaselineOnly      45.932104  \n",
       "CoClustering      53.776164  \n",
       "KNNBaseline        1.718590  \n",
       "KNNBasic           1.732444  \n",
       "KNNWithMeans       1.796209  \n",
       "KNNWithZScore      2.038430  \n",
       "NMF               54.632843  \n",
       "NormalPredictor   56.607932  \n",
       "SlopeOne         479.512936  \n",
       "SVDpp            857.291792  \n",
       "SVD               45.101050  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a list of all the parquet files in the folder\n",
    "file_list = glob.glob(baseline_data + \"/*.parq\")\n",
    "\n",
    "# Read each parquet file and concatenate the dataframes\n",
    "results_df = pd.concat([pd.read_parquet(file) for file in file_list])\n",
    "\n",
    "# saves DataFrame to a CSV file.\n",
    "results_df.to_csv(baseline_data + \"/\" + \"all_models_raseline_results.csv\", index=False)\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rmse                SVD\n",
      "test_mse                 SVD\n",
      "test_mae                 SVD\n",
      "test_fcp            KNNBasic\n",
      "fit_time     NormalPredictor\n",
      "test_time        KNNBaseline\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Find the algorithm with the best result in each column\n",
    "best_algorithms = results_df.idxmin()\n",
    "\n",
    "# Print the results\n",
    "print(best_algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'test_rmse' - 4 lower values:\n",
      "    SVD: 1.132956435783246\n",
      "    SVDpp: 1.1918796500145123\n",
      "    SlopeOne: 1.1965298445727612\n",
      "    BaselineOnly: 1.2057576048718757\n",
      "Column 'test_mse' - 4 lower values:\n",
      "    SVD: 1.283590754047006\n",
      "    SVDpp: 1.4205803687704155\n",
      "    SlopeOne: 1.4316845093318264\n",
      "    BaselineOnly: 1.453852841188779\n",
      "Column 'test_mae' - 4 lower values:\n",
      "    SVD: 0.8447344886103517\n",
      "    SVDpp: 0.8867829101165083\n",
      "    SlopeOne: 0.9037803443753012\n",
      "    CoClustering: 0.9152811379370078\n"
     ]
    }
   ],
   "source": [
    "# Get the best 4 in each column\n",
    "columns = [\"test_rmse\",\"test_mse\",\"test_mae\"]\n",
    "# loop through each column\n",
    "for col in columns:\n",
    "    # sort the column in ascending order\n",
    "    sorted_col = results_df[col].sort_values()\n",
    "    # select the 4 lower values in the column\n",
    "    best_5 = sorted_col[:5]\n",
    "    # print the values with their corresponding names from the index\n",
    "    print(f\"Column '{col}' - 5 lower values:\")\n",
    "    for idx, val in best_5.items():\n",
    "        print(f\"    {idx}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'test_fcp' - 4 highest values:\n",
      "    CoClustering: 0.7111715783286952\n",
      "    SlopeOne: 0.7118410915250115\n",
      "    SVDpp: 0.7307857840551705\n",
      "    SVD: 0.7370923207967957\n"
     ]
    }
   ],
   "source": [
    "# Get the best 4 in each column\n",
    "columns = [\"test_fcp\"]\n",
    "# loop through each column\n",
    "for col in columns:\n",
    "    # sort the column in descending order\n",
    "    sorted_col = results_df[col].sort_values(ascending=True)\n",
    "    # select the 4 highest values in the column\n",
    "    best_5 = sorted_col[-5:]\n",
    "    # print the values with their corresponding names from the index\n",
    "    print(f\"Column '{col}' - 5 highest values:\")\n",
    "    for idx, val in best_5.items():\n",
    "        print(f\"    {idx}: {val}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE or MAE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a recommendation system using the Surprise library, both RMSE (root mean squared error) and MAE (mean absolute error) can be used to evaluate the performance of the model.\n",
    "\n",
    "$$RMSE = \\sqrt {\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y_{i}} - y_{i})^2}$$\n",
    "\n",
    "- **RMSE** measures the average squared difference between the predicted and actual ratings, and it is useful for measuring how well the model is capturing the magnitude of the errors. However, RMSE gives more weight to large errors, which may not be as important in the context of recommendation systems where small errors can still result in meaningful differences in the user's experience.\n",
    "\n",
    "    - When the goal is to minimize the overall error in the predicted ratings in a recommendation system, it means that the objective is to minimize the average difference between the predicted ratings and the actual ratings for all items and users in the system. This can be achieved by optimizing the model to make more accurate predictions for all users and items in the system, regardless of the magnitude of the error.\n",
    "\n",
    "    - Minimizing the overall error in the predicted ratings can be an important goal in recommendation systems, as it can lead to more accurate and personalized recommendations for the users, which can improve their satisfaction and engagement with the system. However, it is important to balance this goal with other factors, such as the diversity, novelty, and serendipity of the recommendations, as well as the computational resources required to generate the recommendations.\n",
    "\n",
    "\n",
    "$$MAE = (\\frac{1}{n})\\sum_{i=1}^{n}\\left | y_{i} - x_{i} \\right |$$\n",
    "\n",
    "- **MAE** measures the average absolute difference between the predicted and actual ratings, and it gives equal weight to all errors regardless of their magnitude. This can be useful in recommendation systems where the goal is to minimize the overall error in the predicted ratings, regardless of whether the errors are large or small.\n",
    "\n",
    "    - When the goal is to focus on minimizing large errors in a recommendation system, it means that the objective is to prioritize reducing the differences between the predicted ratings and the actual ratings for the items and users where the errors are largest. This can be important when the impact of large errors on the user experience is significant or when there are specific items or users where accuracy is particularly important.\n",
    "\n",
    "    - Focusing on minimizing large errors can be useful in certain contexts, such as when there are critical items or users where the accuracy of the recommendations is especially important. However, this approach may also result in overlooking small errors in the prediction, which may accumulate and result in a suboptimal user experience. Therefore, it is important to balance this goal with other factors, such as the overall accuracy, diversity, novelty, and user satisfaction with the recommendations.\n",
    "\n",
    "Therefore, it is recommended to use both RMSE and MAE to evaluate the performance of the model in a recommendation system. However, the choice between the two metrics may depend on the specific requirements of the problem and the priorities of the stakeholders. For example, if the goal is to minimize the overall error in the predicted ratings, MAE might be a better choice. If the goal is to focus on minimizing large errors, RMSE might be more appropriate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Selected models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected the 4 models with best results in the baseline. Now lets evaluate them doing GridSearchCV and training them to get the results.\n",
    "\n",
    "- svd = SVD()\n",
    "- svdp = SVDpp()\n",
    "- baseonly = BaselineOnly()\n",
    "- coclus = CoClustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chrisitan\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os # allows access to OS-dependent functionalities\n",
    "import sys # to manipulate different parts of the Python runtime environment\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Utils libraries\n",
    "from cleaning import *\n",
    "from recommend import *\n",
    "from testing import *\n",
    "from training import *\n",
    "\n",
    "#Preparing folder variables\n",
    "\n",
    "main_folder = os.path.abspath(os.path.join(os.pardir))\n",
    "data_folder = (main_folder + \"/\" +\"data\")\n",
    "saved_models_folder = (data_folder + \"/\" + \"saved_models\")\n",
    "raw_data = (data_folder + \"/\" + \"_raw\")\n",
    "processed_data = (data_folder + \"/\" + \"processed\")\n",
    "baseline_data = (saved_models_folder + \"/\" + \"baseline\")\n",
    "test_models = (saved_models_folder + \"/\" + \"test_models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data, reducing the sample to 1million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_different_models():\n",
    "    '''\n",
    "    The code reads two CSV files (anime.csv and rating.csv.zip) and loads them into dataframes. \n",
    "    Then it creates a subset of the rating dataframe containing only rows where the rating is \n",
    "    greater than 0 and removes the index column. Next, it samples a subset of the data with \n",
    "    a specified size, grouped by the rating column.\n",
    "    '''\n",
    "    # Load 'anime.csv' file into a pandas DataFrame object called 'anime'\n",
    "    anime = pd.read_csv(raw_data + \"/\" + \"anime.csv\")\n",
    "\n",
    "    # Load 'rating.csv.zip' file into a pandas DataFrame object called 'rating'\n",
    "    rating = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "    # Create a new DataFrame 'anime_mapping' that is a copy of the 'anime' DataFrame and remove the 'episodes', 'members', and 'rating' columns\n",
    "    anime_mapping = anime.copy()\n",
    "    anime_mapping.drop(['episodes','members','rating'],axis=1, inplace=True)\n",
    "\n",
    "    # Filter out all ratings less than or equal to 0 and reset the index of the DataFrame\n",
    "    ratingdf = rating[rating.rating>0]\n",
    "    ratingdf = ratingdf.reset_index()\n",
    "\n",
    "    # Drop the 'index' column and update the DataFrame in-place\n",
    "    ratingdf.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    # Get the shape of the DataFrame 'ratingdf'\n",
    "    ratingdf.shape\n",
    "\n",
    "    # Set the size to 1,000,000 and sample from the 'ratingdf' DataFrame based on the proportion of ratings for each score\n",
    "    size = 1000000\n",
    "\n",
    "    # This will make sure that the sampled data has roughly the same proportion of ratings for each score as the original data\n",
    "    ratingdf_sample = ratingdf.groupby(\"rating\", group_keys=False).apply(lambda x: x.sample(int(np.rint(size*len(x)/len(ratingdf))))).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Create a new 'Reader' object with the rating scale set to a range between 1 and 10\n",
    "    reader = Reader(rating_scale=(1,10))\n",
    "\n",
    "    # Load the sampled data into a 'Dataset' object using the 'load_from_df' method and the 'reader' object\n",
    "    data = Dataset.load_from_df(ratingdf_sample[['user_id', 'anime_id', 'rating']], reader)\n",
    "\n",
    "    # Saving the table to pickle\n",
    "    joblib.dump(data,processed_data + \"/\" + \"data_reader_for_different_models.pkl\")\n",
    "\n",
    "    return data\n",
    "data = prepare_for_different_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular Value Decomposition (SVD) is a matrix factorization technique used in recommendation systems to reduce the dimensionality of a user-item matrix and identify latent factors that drive user-item interactions. In essence, SVD represents the original matrix as the product of three matrices: a user matrix, a singular value matrix, and an item matrix. The resulting factors can be used to make personalized recommendations by predicting a user's preference for an item based on their past behavior and the behavior of other similar users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the important hyperparameters for the SVD algorithm in the Surprise library:\n",
    "\n",
    "- **n_factors:** The number of factors to use in the matrix factorization. This controls the number of latent features to be learned from the data.\n",
    "- **n_epochs:** The number of epochs (iterations) to run the matrix factorization algorithm.\n",
    "- **biased:** A boolean indicating whether or not to use biases in the model. Biases represent the average rating for each user and item, and can help improve the accuracy of the predictions.\n",
    "- **lr_all:** The learning rate for all parameters in the model. This controls the step size for each iteration of the optimization algorithm.\n",
    "- **reg_all:** The regularization strength for all parameters in the model. This helps prevent overfitting by adding a penalty term to the optimization objective that encourages the model to have smaller parameter values.\n",
    "- **init_mean:** The mean of the Gaussian distribution used to initialize the factor matrices. By default, this is set to 0.\n",
    "- **init_std_dev:** The standard deviation of the Gaussian distribution used to initialize the factor matrices. By default, this is set to 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of find_best_svd:\n",
    "- Define parameter grid for grid search\n",
    "- Create GridSearchCV object with SVD algorithm\n",
    "- Fit GridSearchCV object to data\n",
    "- Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "- Save model with best parameters\n",
    "- Save best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score: 1.1183239072148927\n",
      "Best MAE score: 0.8364221843283999\n",
      "Best parameters for RMSE: {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.05}\n",
      "Best parameters for MAE: {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.005, 'reg_all': 0.05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.model_selection.search.GridSearchCV at 0x1bd2c815b80>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best_svd(data):\n",
    "    '''\n",
    "    defines a parameter grid for hyperparameter tuning in a collaborative filtering algorithm.\n",
    "    Then create a GridSearchCV object with the SVD algorithm and a parameter grid consisting \n",
    "    of a range of hyperparameters. The GridSearchCV function then performs a grid search on \n",
    "    yhe parameter grid to find the best combination of hyperparameters that minimizes the \n",
    "    RMSE and MAE scores. The best RMSE and MAE scores and the corresponding parameters \n",
    "    are printed out.\n",
    "    '''\n",
    "    from surprise import SVD\n",
    "    from surprise.model_selection import GridSearchCV\n",
    "    data = data\n",
    "\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {'n_factors': [50, 100, 150, 300], \n",
    "                'n_epochs': [20, 30, 40, 50], \n",
    "                'lr_all': [0.002, 0.005, 0.01, 0.1],\n",
    "                'reg_all': [0.02, 0.05, 0.1]}\n",
    "\n",
    "    # Create GridSearchCV object with SVD algorithmr\n",
    "    gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae','mse'], cv=5)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "    \n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"SVD_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"SVD_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    return gs\n",
    "best_params = find_best_svd(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVDpp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD++ is an extension of Singular Value Decomposition (SVD) that takes into account implicit feedback in recommendation systems. In addition to the user-item matrix used in SVD, SVD++ also considers a matrix of implicit feedback such as user interactions with items, item attributes, and user preferences. This additional matrix helps capture the influence of user and item biases on the recommendation process, resulting in more accurate and personalized recommendations. SVD++ also includes a regularization term to avoid overfitting and improve generalization. Overall, SVD++ is a more advanced and sophisticated technique for recommendation systems than SVD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVDpp algorithm in the Surprise library has the following hyperparameters:\n",
    "\n",
    "- **n_factors:** the number of latent factors. Default is 20.\n",
    "- **n_epochs:** the number of iterations for the optimization algorithms. Default is 20.\n",
    "- **lr_all:** the learning rate for all parameters. Default is 0.005.\n",
    "- **reg_all:** the regularization parameter for all parameters. Default is 0.02.\n",
    "- **init_mean:** the mean of the normal distribution for factor vectors initialization. Default is 0.\n",
    "- **init_std_dev:** the standard deviation of the normal distribution for factor vectors initialization. Default is 0.1.\n",
    "- **verbose:** whether to print details during the optimization process. Default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "def find_best_svdpp(data):\n",
    "    from surprise import SVDpp\n",
    "    from surprise.model_selection import GridSearchCV\n",
    "    import joblib\n",
    "\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {'n_factors': [50, 100, 150], \n",
    "                  'n_epochs': [20, 30, 40], \n",
    "                  'lr_all': [0.002, 0.005, 0.01],\n",
    "                  'reg_all': [0.02, 0.05, 0.1]\n",
    "                  }\n",
    "\n",
    "    # Create GridSearchCV object with SVDpp algorithm\n",
    "    gs = GridSearchCV(SVDpp, param_grid, measures=['rmse', 'mae','mse'], cv=5)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"SVDpp_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"SVDpp_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    return gs\n",
    "find_best_svdpp(data)\n",
    "\n",
    "#### Saving the output of this cell to a file\n",
    "# Check if the file exists\n",
    "if os.path.exists(os.path.join(test_models + \"/\" + \"capture_SVDpp.txt\")):\n",
    "\n",
    "    # If the file exists, open it in \"a\" mode to append to the end\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_SVDpp.txt\"), \"a\") as f:\n",
    "        \n",
    "        # Write the captured output to the end of the file\n",
    "        f.write(cap.stdout)       \n",
    "else:\n",
    "    # If the file does not exist, create it and open it in \"w\" mode to write to it\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_SVDpp.txt\"), \"w\") as f:\n",
    "\n",
    "        # Write the captured output to the file\n",
    "        f.write(cap.stdout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaselineOnly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaselineOnly is a simple but effective collaborative filtering algorithm used in recommendation systems. It predicts a user's rating for an item by taking into account the overall average rating of all items, the average rating of the user, and the average rating of the item. These three values are used as baseline estimates, and the difference between the actual rating and the baseline estimate is used as the prediction error. BaselineOnly then learns user and item biases that can improve the accuracy of the baseline estimates. The algorithm is simple and computationally efficient, making it a popular choice for recommendation systems with large datasets. However, it may not capture more complex relationships between users and items compared to more advanced techniques such as matrix factorization.BaselineOnly is a simple but effective collaborative filtering algorithm used in recommendation systems. It predicts a user's rating for an item by taking into account the overall average rating of all items, the average rating of the user, and the average rating of the item. These three values are used as baseline estimates, and the difference between the actual rating and the baseline estimate is used as the prediction error. BaselineOnly then learns user and item biases that can improve the accuracy of the baseline estimates. The algorithm is simple and computationally efficient, making it a popular choice for recommendation systems with large datasets. However, it may not capture more complex relationships between users and items compared to more advanced techniques such as matrix factorization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaselineOnly algorithm in the Surprise library has the following hyperparameters:\n",
    "- **bsl_options:** a dictionary containing the following options for the baseline estimates:\n",
    "    - **method:** the method used to compute the baseline estimates. Possible values are: als, sgd. Default is als.\n",
    "    - **n_epochs:** the number of iterations for the optimization algorithms. Default is 20.\n",
    "    - **reg_u:** the regularization parameter for users. Default is 15.\n",
    "    - **reg_i:** the regularization parameter for items. Default is 10.\n",
    "    - **verbose**: whether to print details during the optimization process. Default is False.\n",
    "- **biased:** whether to include the baseline estimate in the prediction. Default is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "def find_best_BaselineOnly(data):\n",
    "    from surprise import BaselineOnly\n",
    "\n",
    "    from surprise.model_selection import GridSearchCV\n",
    "    import joblib\n",
    "\n",
    "    # Parameters docs and value ranges:\n",
    "    # http://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baseline-estimates-configuration\n",
    "    # http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf\n",
    " \n",
    "     # Define parameter grid for grid search\n",
    "\n",
    "    param_grid = {'bsl_options': {'method': ['sgd', 'als'],\n",
    "                                'reg': [0.02, 0.05, 0.1],\n",
    "                                'learning_rate': [0.001, 0.005, 0.01],\n",
    "                                'n_epochs': [5, 10, 15],\n",
    "                                'verbose': [True]},\n",
    "                'verbose': [True]}\n",
    "\n",
    "    # Create GridSearchCV object with SVDpp algorithm\n",
    "    gs = GridSearchCV(BaselineOnly, param_grid, measures=['rmse', 'mae','mse'], cv=10, n_jobs=1)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"BaselineOnly_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"BaselineOnly_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    \n",
    "    return gs\n",
    "\n",
    "find_best_BaselineOnly(data)\n",
    "\n",
    "#### Saving the output of this cell to a file\n",
    "# Check if the file exists\n",
    "if os.path.exists(os.path.join(test_models + \"/\" + \"capture_BaselineOnly.txt\")):\n",
    "\n",
    "    # If the file exists, open it in \"a\" mode to append to the end\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_BaselineOnly.txt\"), \"a\") as f:\n",
    "        \n",
    "        # Write the captured output to the end of the file\n",
    "        f.write(cap.stdout)       \n",
    "else:\n",
    "    # If the file does not exist, create it and open it in \"w\" mode to write to it\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_BaselineOnly.txt\"), \"w\") as f:\n",
    "\n",
    "        # Write the captured output to the file\n",
    "        f.write(cap.stdout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best RMSE score: 1.5566270718662043\n",
    "- Best MSE score: 2.426553220392917\n",
    "- Best MAE score: 1.2098539052864254\n",
    "- Best parameters for RMSE: {'bsl_options': {'method': 'sgd', 'reg': 0.02, 'learning_rate': 0.01, 'n_epochs': 15, 'verbose': True}, 'verbose': True}\n",
    "- Best parameters for MAE: {'bsl_options': {'method': 'sgd', 'reg': 0.02, 'learning_rate': 0.01, 'n_epochs': 15, 'verbose': True}, 'verbose': True}\n",
    "- Best parameters for MSE: {'bsl_options': {'method': 'sgd', 'reg': 0.02, 'learning_rate': 0.01, 'n_epochs': 15, 'verbose': True}, 'verbose': True}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoClustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoClustering is a collaborative filtering algorithm for recommendation systems that groups users and items into clusters and then estimates the ratings based on the interactions between these clusters. The algorithm tries to find a block diagonal structure in the user-item matrix, where users and items are clustered together. This approach can be more effective than traditional matrix factorization techniques in cases where users or items have similar tastes or properties. The CoClustering algorithm is available in the Surprise library, which is a popular Python library for building and evaluating recommendation systems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief explanation of these hyperparameters:\n",
    "\n",
    "- **n_cltr_u** and **n_cltr_i:** the number of user and item clusters, respectively. This is a crucial hyperparameter for CoClustering, as it determines the level of granularity of the clustering. Higher values will generally result in better performance but may also increase training time.\n",
    "- **n_epochs:** the number of epochs or iterations to run the algorithm for. This is also an important hyperparameter, as it determines the number of times the algorithm will iterate over the data. More iterations can lead to better performance but may also increase training time.\n",
    "- **verbose:** a boolean flag indicating whether or not to print progress messages during training.\n",
    "- **random_state:** an integer value representing the random seed used to initialize the algorithm's parameters. This can be useful for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise libraries\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "from surprise import CoClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "def find_best_coclustering(data):\n",
    "\n",
    "\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {'n_cltr_u': [3, 5, 10], \n",
    "                'n_cltr_i': [3, 5, 10], \n",
    "                'n_epochs': [20, 30, 40], \n",
    "                'verbose': [True, False], \n",
    "                'random_state': [42, 123]\n",
    "                }\n",
    "\n",
    "    # Create GridSearchCV object with SVDpp algorithm\n",
    "    gs = GridSearchCV(CoClustering, param_grid, measures=['rmse', 'mae','mse'], cv=5)\n",
    "\n",
    "    # Fit GridSearchCV object to data\n",
    "    gs.fit(data)\n",
    "\n",
    "    # Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "    print(\"Best RMSE score:\", gs.best_score['rmse'])\n",
    "    print(\"Best MSE score:\", gs.best_score['mse'])\n",
    "    print(\"Best MAE score:\", gs.best_score['mae'])\n",
    "    print(\"Best parameters for RMSE:\", gs.best_params['rmse'])\n",
    "    print(\"Best parameters for MAE:\", gs.best_params['mae'])\n",
    "    print(\"Best parameters for MSE:\", gs.best_params['mse'])\n",
    "\n",
    "    # Save model with best parameters\n",
    "    joblib.dump(gs,test_models + \"/\" + \"CoClustering_test_model.pkl\")\n",
    "\n",
    "    # Save best parameters\n",
    "    joblib.dump(gs.best_params,test_models + \"/\" + \"CoClustering_best_params_test_model.pkl\", compress = 1)\n",
    "\n",
    "    return gs\n",
    "find_best_coclustering(data)\n",
    "\n",
    "\n",
    "#### Saving the output of this cell to a file\n",
    "# Check if the file exists\n",
    "if os.path.exists(os.path.join(test_models + \"/\" + \"capture_CoClustering.txt\")):\n",
    "\n",
    "    # If the file exists, open it in \"a\" mode to append to the end\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_CoClustering.txt\"), \"a\") as f:\n",
    "        \n",
    "        # Write the captured output to the end of the file\n",
    "        f.write(cap.stdout)       \n",
    "else:\n",
    "    # If the file does not exist, create it and open it in \"w\" mode to write to it\n",
    "    with open(os.path.join(test_models + \"/\" + \"capture_CoClustering.txt\"), \"w\") as f:\n",
    "\n",
    "        # Write the captured output to the file\n",
    "        f.write(cap.stdout)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and training the final selected SVD model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In roder to evaluate, select and train the desired model, we will use 3 different funcions from clening.py, testing.py and testing.py in utils folder:\n",
    "- supervised_prepare_training\n",
    "- find_best_svd\n",
    "- train_test_svd\n",
    "- svd_precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(supervised_prepare_training.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to:\n",
    "- Load 'anime.csv' file into a pandas DataFrame object called 'anime'\n",
    "- Load 'rating.csv.zip' file into a pandas DataFrame object called 'rating'\n",
    "- Create a new DataFrame 'anime_mapping' that is a copy of the 'anime' DataFrame and remove the 'episodes', 'members', and 'rating' columns\n",
    "- Filter out all ratings less than or equal to 0 and reset the index of the DataFrame\n",
    "- Drop the 'index' column and update the DataFrame in-place\n",
    "- Get the shape of the DataFrame 'ratingdf'\n",
    "- Set the size to 100,000 and sample from the 'ratingdf' DataFrame based on the proportion of ratings for each score\n",
    "- This will make sure that the sampled data has roughly the same proportion of ratings for each score as the original data\n",
    "- Create a new 'Reader' object with the rating scale set to a range between 1 and 10\n",
    "- Load the sampled data into a 'Dataset' object using the 'load_from_df' method and the 'reader' object\n",
    "- Saving the table to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x2cafb97a640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "supervised_prepare_training() #utils.cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(find_best_svd.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of find_best_svd:\n",
    "- Define parameter grid for grid search\n",
    "- Create GridSearchCV object with SVD algorithm\n",
    "- Fit GridSearchCV object to data\n",
    "- Print best RMSE and MAE scores, as well as corresponding parameters\n",
    "- Save model with best parameters\n",
    "- Save best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score: 1.3805614676765472\n",
      "Best MAE score: 1.0649921646188643\n",
      "Best parameters for RMSE: {'n_factors': 50, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1}\n",
      "Best parameters for MAE: {'n_factors': 50, 'n_epochs': 40, 'lr_all': 0.01, 'reg_all': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.model_selection.search.GridSearchCV at 0x2cafb917ac0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_svd() #utils.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_test_svd.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of train_test_svd:\n",
    "- Loads the best hyperparameters for the SVD algorithm that were obtained from grid search\n",
    "- Loads the dataset from a pickle file using joblib\n",
    "- Splits the data into training and testing sets with a 80:20 ratio\n",
    "- Creates an instance of the SVD algorithm with the best hyperparameters obtained from grid search\n",
    "- Trains the SVD algorithm on the training set using the fit() method\n",
    "- Generates predictions for the test set using the trained model\n",
    "- Calculates the RMSE and MAE for the predictions\n",
    "- Saves the trained model as a pickle file using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3757\n",
      "MAE:  1.0561\n",
      "RMSE: 1.375737289908081\n",
      "MAE: 1.056142242807008\n"
     ]
    }
   ],
   "source": [
    "train_test_svd() #utils.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svd_precision_recall.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_precision_recall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the recommendations we will use the next functions from recommend.py in utils folder:\n",
    "- sort_it\n",
    "- create_dict_su\n",
    "- filtering_and\n",
    "- filtering_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sort_it.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of sort_it:\n",
    "- Load the pre-trained SVD model   \n",
    "- Load the anime dataframe  \n",
    "- Apply the SVD model to estimate the score for each anime\n",
    "- Sort the dataframe by the estimated score in descending order and drop the anime_id column\n",
    "- Create a blank index for the dataframe\n",
    "- Set the blank index to the dataframe\n",
    "- Return the sorted dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_dict_su.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of create_dict_su:\n",
    "- get the final dataframe and the parameters for filtering and number of recommendations to show\n",
    "- check which method was used to filter the recommendations, 'or' or 'and'\n",
    "\t- filter the dataframe using the OR logic and the given genres and types\n",
    "\t- filter the dataframe using the AND logic and the given genres and types\n",
    "\t- raise an error if an invalid filter type was given     \n",
    "- select the top n recommendations from the filtered dataframe\n",
    "- if the filtered dataframe is empty, print a message\n",
    "\t- convert the filtered dataframe to a dictionary\n",
    "\t- return the dictionary of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function takes a DataFrame df, a list of genres, and a list of types as input arguments. \n",
      "    The function first creates a boolean mask genre_mask by applying a lambda function to \n",
      "    the 'genre' column of the DataFrame. The lambda function checks if the value is a \n",
      "    string using isinstance(x, str) and if all genres in the genres list are present \n",
      "    in the string, which is split by comma and space using x.split(', '). \n",
      "    The all() function returns True if all genres in the genres list are present \n",
      "    in the string. The resulting genre_mask will be True for rows where the genre \n",
      "    column contains all of the genres in the genres list.\n",
      "\n",
      "    Then the function creates another boolean mask type_mask by using the isin() \n",
      "    method to check if each value in the 'type' column of the DataFrame is in the types list.\n",
      "\n",
      "    Finally, the function applies both masks to the DataFrame df using the & operator \n",
      "    to create a new DataFrame filtered_df that includes only rows where both m\n",
      "    asks are True. The function returns the filtered DataFrame.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(filtering_and.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of filtering_and:\n",
    "- This function takes a DataFrame `df`, a list of `genres`, and a list of `types` as input arguments.\n",
    "- Create a boolean mask that filters rows where the genre column contains all of the genres in the `genres` list.\n",
    "- Create a boolean mask that filters rows where the type column is in the `types` list.\n",
    "- Apply both masks to the DataFrame `df` and create a new DataFrame `filtered_df` that includes only rows where both masks are True.\n",
    "- Return the filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The code defines a function \"filtering_or\" that filters a pandas dataframe based on user-defined \n",
      "    genres and types using an \"OR\" method. The function allows the user to select one or all possible \n",
      "    genres and types and returns a filtered dataframe with the selected genres and types. \n",
      "    The function also splits the genre and type columns and explodes them to account for multiple entries.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(filtering_or.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of filtering_or:\n",
    "- Make a copy of the input DataFrame\n",
    "- Split the genre column into a list of genres\n",
    "- Explode the genre column to create a new row for each genre in the list\n",
    "- If genres are specified and 'ALL' is not one of them, filter the DataFrame to keep only rows where the genre is in the specified list  \n",
    "- If types are specified and 'ALL' is not one of them, filter the DataFrame to keep only rows where the type is in the specified list\n",
    "- If both genres and types are specified\n",
    "- If 'ALL' is in the genres list, set genres to be all the unique genres in the filtered DataFrame\n",
    "- If 'ALL' is in the types list, set types to be all the unique types in the filtered DataFrame\n",
    "- Filter the DataFrame to keep only rows where the genre is in the genres list AND the type is in the types list\n",
    "- Return the filtered DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Fullmetal Alchemist: Brotherhood',\n",
       "  'english_title': 'Fullmetal Alchemist: Brotherhood',\n",
       "  'japanses_title': '鋼の錬金術師 FULLMETAL ALCHEMIST',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': '64',\n",
       "  'rating': 'R - 17+ (violence & profanity)',\n",
       "  'score': 9.11,\n",
       "  'rank': 1.0,\n",
       "  'members': 793665,\n",
       "  'synopsis': 'After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse\\'s body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse\\'s soul in the physical realm by binding it to a hulking suit of armor.\\r\\n\\r\\nThe brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing \"automail,\" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher\\'s Stone—a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.\\r\\n\\r\\nAs Edward becomes an infamous alchemist and gains the nickname \"Fullmetal,\" the boys\\' journey embroils them in a growing conspiracy that threatens the fate of the world.\\r\\n\\r\\n[Written by MAL Rewrite]',\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/1208/94745l.jpg',\n",
       "  'Estimate_Score': 9.240005320221256},\n",
       " {'name': 'Gintama°',\n",
       "  'english_title': 'Gintama°',\n",
       "  'japanses_title': '銀魂°',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': '51',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 9.07,\n",
       "  'rank': 4.0,\n",
       "  'members': 114262,\n",
       "  'synopsis': \"Gintoki, Shinpachi, and Kagura return as the fun-loving but broke members of the Yorozuya team! Living in an alternate-reality Edo, where swords are prohibited and alien overlords have conquered Japan, they try to thrive on doing whatever work they can get their hands on. However, Shinpachi and Kagura still haven't been paid... Does Gin-chan really spend all that cash playing pachinko?\\r\\n\\r\\nMeanwhile, when Gintoki drunkenly staggers home one night, an alien spaceship crashes nearby. A fatally injured crew member emerges from the ship and gives Gintoki a strange, clock-shaped device, warning him that it is incredibly powerful and must be safeguarded. Mistaking it for his alarm clock, Gintoki proceeds to smash the device the next morning and suddenly discovers that the world outside his apartment has come to a standstill. With Kagura and Shinpachi at his side, he sets off to get the device fixed; though, as usual, nothing is ever that simple for the Yorozuya team.\\r\\n\\r\\nFilled with tongue-in-cheek humor and moments of heartfelt emotion, Gintama's fourth season finds Gintoki and his friends facing both their most hilarious misadventures and most dangerous crises yet.\\r\\n\\r\\n[Written by MAL Rewrite]\",\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/3/72078l.jpg',\n",
       "  'Estimate_Score': 9.197089662352965},\n",
       " {'name': 'Haikyuu!!: Karasuno Koukou VS Shiratorizawa Gakuen Koukou',\n",
       "  'english_title': 'Haikyuu!! Karasuno Koukou vs. Shiratorizawa Gakuen Koukou',\n",
       "  'japanses_title': 'ハイキュー!! 烏野高校 VS 白鳥沢学園高校',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': '10',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 8.78,\n",
       "  'rank': 35.0,\n",
       "  'members': 93351,\n",
       "  'synopsis': 'After the victory against Aoba Jousai High, Karasuno High School, once called “a fallen powerhouse, a crow that can’t fly,” has finally reached the climax of the heated Spring tournament. Now, to advance to nationals, the Karasuno team has to defeat the powerhouse Shiratorizawa Academy. Karasuno’s greatest hurdle is their adversary’s ace, Wakatoshi Ushijima, the number one player in the Miyagi Prefecture, and one of the country’s top three aces.\\r\\n\\r\\nOnly the strongest team will make it to the national tournament. Since this match is the third-year players’ last chance to qualify for nationals, Karasuno has to use everything they learned during the training camp and prior matches to attain victory. Filled with restlessness and excitement, both teams are determined to come out on top in the third season of Haikyuu!!.\\r\\n\\r\\n[Written by MAL Rewrite]',\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/7/81992l.jpg',\n",
       "  'Estimate_Score': 9.191643739956707},\n",
       " {'name': 'Hunter x Hunter (2011)',\n",
       "  'english_title': 'Hunter x Hunter (2011)',\n",
       "  'japanses_title': 'HUNTER×HUNTER（ハンター×ハンター）',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '23 min per ep',\n",
       "  'episodes': '148',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 9.04,\n",
       "  'rank': 9.0,\n",
       "  'members': 425855,\n",
       "  'synopsis': \"Hunters devote themselves to accomplishing hazardous tasks, all from traversing the world's uncharted territories to locating rare items and monsters. Before becoming a Hunter, one must pass the Hunter Examination—a high-risk selection process in which most applicants end up handicapped or worse, deceased.\\r\\n\\r\\nAmbitious participants who challenge the notorious exam carry their own reason. What drives 12-year-old Gon Freecss is finding Ging, his father and a Hunter himself. Believing that he will meet his father by becoming a Hunter, Gon takes the first step to walk the same path.\\r\\n\\r\\nDuring the Hunter Examination, Gon befriends the medical student Leorio Paladiknight, the vindictive Kurapika, and ex-assassin Killua Zoldyck. While their motives vastly differ from each other, they band together for a common goal and begin to venture into a perilous world.\\r\\n\\r\\n[Written by MAL Rewrite]\",\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/1337/99013l.jpg',\n",
       "  'Estimate_Score': 9.166091462407456},\n",
       " {'name': 'Gintama&#039;: Enchousen',\n",
       "  'english_title': \"Gintama': Enchousen\",\n",
       "  'japanses_title': \"銀魂' 延長戦\",\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': '13',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 9.03,\n",
       "  'rank': 10.0,\n",
       "  'members': 81109,\n",
       "  'synopsis': \"While Gintoki Sakata was away, the Yorozuya found themselves a new leader: Kintoki, Gintoki's golden-haired doppelganger. In order to regain his former position, Gintoki will need the help of those around him, a troubling feat when no one can remember him! Between Kintoki and Gintoki, who will claim the throne as the main character?\\r\\n\\r\\nIn addition, Yorozuya make a trip back down to red-light district of Yoshiwara to aid an elderly courtesan in her search for her long-lost lover. Although the district is no longer in chains beneath the earth's surface, the trio soon learn of the tragic backstories of Yoshiwara's inhabitants that still haunt them. With flashback after flashback, this quest has Yorozuya witnessing everlasting love and protecting it as best they can with their hearts and souls.\\r\\n\\r\\n[Written by MAL Rewrite]\",\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/1452/123686l.jpg',\n",
       "  'Estimate_Score': 9.16261420396817},\n",
       " {'name': 'Gintama',\n",
       "  'english_title': 'Gintama',\n",
       "  'japanses_title': '銀魂',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': '201',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 8.94,\n",
       "  'rank': 17.0,\n",
       "  'members': 336376,\n",
       "  'synopsis': 'Edo is a city that was home to the vigor and ambition of samurai across the country. However, following feudal Japan\\'s surrender to powerful aliens known as the \"Amanto,\" those aspirations now seem unachievable. With the once-influential shogunate rebuilt as a puppet government, a new law is passed that promptly prohibits all swords in public. \\r\\n\\r\\nEnter Gintoki Sakata, an eccentric silver-haired man who always carries around a wooden sword and maintains his stature as a samurai despite the ban. As the founder of Yorozuya, a small business for odd jobs, Gintoki often embarks on endeavors to help other people—though usually in rather strange and unforeseen ways. \\r\\n\\r\\nAssisted by Shinpachi Shimura, a boy with glasses supposedly learning the way of the samurai; Kagura, a tomboyish girl with superhuman strength and an endless appetite; and Sadaharu, their giant pet dog who loves biting on people\\'s heads, the Yorozuya encounter anything from alien royalty to scuffles with local gangs in the ever-changing world of Edo.\\r\\n\\r\\n[Written by MAL Rewrite]',\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/10/73274l.jpg',\n",
       "  'Estimate_Score': 9.150980537216345},\n",
       " {'name': 'Gintama&#039;',\n",
       "  'english_title': \"Gintama'\",\n",
       "  'japanses_title': \"銀魂'\",\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': '51',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 9.04,\n",
       "  'rank': 8.0,\n",
       "  'members': 151266,\n",
       "  'synopsis': \"After a one-year hiatus, Shinpachi Shimura returns to Edo, only to stumble upon a shocking surprise: Gintoki and Kagura, his fellow Yorozuya members, have become completely different characters! Fleeing from the Yorozuya headquarters in confusion, Shinpachi finds that all the denizens of Edo have undergone impossibly extreme changes, in both appearance and personality. Most unbelievably, his sister Otae has married the Shinsengumi chief and shameless stalker Isao Kondou and is pregnant with their first child.\\r\\n\\r\\nBewildered, Shinpachi agrees to join the Shinsengumi at Otae and Kondou's request and finds even more startling transformations afoot both in and out of the ranks of the the organization. However, discovering that Vice Chief Toushirou Hijikata has remained unchanged, Shinpachi and his unlikely Shinsengumi ally set out to return the city of Edo to how they remember it.\\r\\n\\r\\nWith even more dirty jokes, tongue-in-cheek parodies, and shameless references, Gintama' follows the Yorozuya team through more of their misadventures in the vibrant, alien-filled world of Edo.\\r\\n\\r\\n[Written by MAL Rewrite]\",\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/4/50361l.jpg',\n",
       "  'Estimate_Score': 9.096140703215436},\n",
       " {'name': 'Aria The Origination',\n",
       "  'english_title': 'Aria the Origination',\n",
       "  'japanses_title': 'ARIA The ORIGINATION',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': '13',\n",
       "  'rating': 'G - All Ages',\n",
       "  'score': 8.49,\n",
       "  'rank': 130.0,\n",
       "  'members': 56162,\n",
       "  'synopsis': 'In the 24th century on the planet Aqua, three girls—Akari Mizunashi, Alice Carroll, and Aika S. Granzchesta—continue to work hard toward achieving their goal of becoming Prima Undines: professional tour guide gondoliers. Luckily, the girls have the guidance of the three best Prima Undines in Neo-Venezia—Alicia Florence, Athena Glory, and Akira E. Ferrari—who are known as the \"Water Fairies\" in honor of their skill. With their help, the young apprentices train hard and work to overcome any situations that they find themselves in.\\r\\n\\r\\nAria The Origination follows the hardships and daily lives of these three young girls, who are doing their best to improve as tour gondoliers in Neo-Venezia, a terraformed replica of Venice.\\r\\n\\r\\n[Written by MAL Rewrite]',\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/1394/129606l.jpg',\n",
       "  'Estimate_Score': 8.91693800648121},\n",
       " {'name': 'Hajime no Ippo',\n",
       "  'english_title': 'Hajime no Ippo',\n",
       "  'japanses_title': 'はじめの一歩 THE FIGHTING!',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '23 min per ep',\n",
       "  'episodes': '75',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 8.75,\n",
       "  'rank': 41.0,\n",
       "  'members': 157670,\n",
       "  'synopsis': \"In his father's absence, teenager Ippo Makunouchi works hard to help his mother run her fishing boat rental business. Ippo's timid nature, his lack of sleep, and the sea smell make him an easy target for relentless bullies who leave him bruised and beaten on a daily basis. Mamoru Takamura, an up-and-coming boxer, rescues Ippo from a violent after-school incident and takes him back to the Kamogawa Boxing Gym for recovery. Takamura and his fellow boxers, Masaru Aoki and Tatsuya Kimura, are stunned by Ippo's powerful punches—a result of strong muscles developed through years serving his physically taxing family business. \\r\\n\\r\\nFollowing brief training under Takamura, Ippo impresses the other boxers in a practice match against prodigy Ichirou Miyata. He gains a rival in Miyata and a coach in Genji Kamogawa, the gym owner and a former boxer himself. As Ippo takes the first steps in his official boxing career, he faces off against a series of challenging opponents, each more powerful than the last. Victory, loss, and a cycle of dedicated training await Ippo on his journey to achieve greatness. With his tough body and unstoppable fighting spirit, the kind young man seeks to take on the world.\\r\\n\\r\\n[Written by MAL Rewrite]\",\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/4/86334l.jpg',\n",
       "  'Estimate_Score': 8.91608397569709},\n",
       " {'name': 'Shigatsu wa Kimi no Uso',\n",
       "  'english_title': 'Shigatsu wa Kimi no Uso',\n",
       "  'japanses_title': '四月は君の嘘',\n",
       "  'genre': 'Shounen',\n",
       "  'type': 'TV',\n",
       "  'source': 'Manga',\n",
       "  'duration': '22 min per ep',\n",
       "  'episodes': '22',\n",
       "  'rating': 'PG-13 - Teens 13 or older',\n",
       "  'score': 8.65,\n",
       "  'rank': 68.0,\n",
       "  'members': 416397,\n",
       "  'synopsis': 'Kousei Arima is a child prodigy known as the \"Human Metronome\" for playing the piano with precision and perfection. Guided by a strict mother and rigorous training, Kousei dominates every competition he enters, earning the admiration of his musical peers and praise from audiences. When his mother suddenly passes away, the subsequent trauma makes him unable to hear the sound of a piano, and he never takes the stage thereafter.\\r\\n\\r\\nNowadays, Kousei lives a quiet and unassuming life as a junior high school student alongside his friends Tsubaki Sawabe and Ryouta Watari. While struggling to get over his mother\\'s death, he continues to cling to music. His monochrome life turns upside down the day he encounters the eccentric violinist Kaori Miyazono, who thrusts him back into the spotlight as her accompanist. Through a little lie, these two young musicians grow closer together as Kaori tries to fill Kousei\\'s world with color.\\r\\n\\r\\n[Written by MAL Rewrite]',\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/3/67177l.jpg',\n",
       "  'Estimate_Score': 8.86332360872485}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the recommendation as a dictionary\n",
    "# We input the user ID for we want the recommendations\n",
    "# Then the genre we want (or write \"All\" if we shoose \"or\" filter)\n",
    "# Then the type we want (or write \"All\" if we shoose \"or\" filter)\n",
    "# We must select a type or filtering, \"or\"/\"and\" \n",
    "# Then the number of suggestions we have(we might get less if there not so many o none if there is no matches)\n",
    "\n",
    "create_dict_su(sort_it(25000),[\"Shounen\"],[\"TV\"],\"or\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b81dc10ae865b1cfc2801720682109336e501398cc38da3c954913355cae8fad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
