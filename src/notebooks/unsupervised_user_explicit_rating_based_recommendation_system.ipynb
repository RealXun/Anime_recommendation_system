{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised user explicit rating based recommendation system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chrisitan\\miniconda3\\envs\\stlit\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os # allows access to OS-dependent functionalities\n",
    "import sys # to manipulate different parts of the Python runtime environment\n",
    "\n",
    "import numpy as np # functions for working in domain of linear algebra, fourier transform, matrices and arrays\n",
    "import pandas as pd # data analysis and manipulation tool\n",
    "\n",
    "# setting display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Utils libraries\n",
    "from cleaning import *\n",
    "from recommend import *\n",
    "from testing import *\n",
    "from training import *\n",
    "\n",
    "#Preparing folder variables\n",
    "\n",
    "main_folder = os.path.abspath(os.path.join(os.pardir))\n",
    "data_folder = (main_folder + \"/\" +\"data\")\n",
    "saved_models_folder = (data_folder + \"/\" + \"saved_models\")\n",
    "raw_data = (data_folder + \"/\" + \"_raw\")\n",
    "processed_data = (data_folder + \"/\" + \"processed\")\n",
    "content_based_supervised_data = (main_folder + \"/\" + \"processed\" + \"/\" + \"content_based_supervised\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preparing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file called \"anime.csv\" from a directory called raw_data and returns the contents as a Pandas DataFrame\n",
    "anime = pd.read_csv(raw_data + \"/\" + \"anime.csv\") \n",
    "\n",
    "# CSV file called \"rating.csv.zip\" from a directory called raw_data and returns the contents as a Pandas DataFrame\n",
    "rating = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Merging dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that we will call the next functions from cleaning.py in utils folder.\n",
    "- final_df\n",
    "- clean_anime_df\n",
    "    - predict_source\n",
    "    - clean_synopsis\n",
    "- merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function merges the two dataframes of anime data, reorders and selects columns, \n",
      "    renames the columns to lowercase, and saves the resulting dataframe to a CSV file. \n",
      "    The merged and cleaned dataframe is returned as the output.\n",
      "    In other words, we get more information like to get more information like cover or japanese tittle\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(final_df.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function are:\n",
    "- Load the original anime dataframe\n",
    "- Load the updated anime dataframe\n",
    "- Merge the two dataframes on the anime_id column \n",
    "- Reorder and select columns \n",
    "- Rename columns to lower case \n",
    "- Save the final dataframe to a CSV file in the processed data directory\n",
    "- Return the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function clean_anime_df() takes an anime dataframe as input and performs several \n",
      "    cleaning and preprocessing steps, such as removing special characters from anime names, \n",
      "    converting all names to lowercase, filling missing values for \"episodes\" and \"score\" \n",
      "    columns with their median, dropping rows with null values for \"genre\" or \"type\" columns, \n",
      "    and saving the cleaned dataframe to a CSV file. The cleaned dataframe is also returned as output.\n"
     ]
    }
   ],
   "source": [
    "print(clean_anime_df.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function:\n",
    "- Create a copy of the original dataframe called anime_cleaned\n",
    "- Remove all non-word characters from the name column and replace them with spaces\n",
    "- Convert all names to lowercase\n",
    "- Replace all \"Unknown\" values in the episodes column with NaN\n",
    "- Replace all NaN values in the episodes column with the median of the column\n",
    "- Convert the score column to float type\n",
    "- Replace all NaN values in the score column with the median of the column\n",
    "- Convert the members column to float type\n",
    "- Apply the clean_synopsis function to the synopsis column\n",
    "    - Remove \\r and \\n from synopsis\n",
    "    - Remove extra spaces from synopsis\n",
    "    - Replace encoded characters\n",
    "    - Return synopsis\n",
    "- Add prediction to the source column of the dataframe using the predict_source function\n",
    "    - change unknown values to NaN from 'source' column\n",
    "    - fill missing values in the 'episodes' column with 0\n",
    "    - create dummy variables for the 'type' column\n",
    "    - create dummy variables for the 'rating' column\n",
    "    - First, we area going to split the genre column by comma, then expand the list, so there is a column for each genre. We will have 13 columns, because the anime with most genres tags has 13 tags\n",
    "    - Now we can get the list of unique genres. We \"convert\" the dataframe into a single dimension array and take the unique values\n",
    "    - Getting the dummy variables will result in having a lot more columns than unique genres\n",
    "    - So we sum up the columns with the same genre to have a single column for each genre\n",
    "    - split the data into training and validation sets\n",
    "    - create the decision tree classifier\n",
    "    - train the model using the training data\n",
    "    - predict the 'source' values for the validation data\n",
    "    - fill the 'NaN' 'source' values in the original DataFrame with the predicted values\n",
    "    - undo the get_dummies() operation to convert the one-hot encoded 'type' and 'rating' columns back to a single categorical column\n",
    "    - Dropping unnecessary columns\n",
    "    - calculate the accuracy of the model\n",
    "- Replace all NaN values in the genre column with the mode of the column\n",
    "- Replace all NaN values in the rating column with the mode of the column\n",
    "- Replace all NaN values in the type column with the mode of the column\n",
    "- Save the cleaned dataframe to a CSV file called \"_anime_to_compare_with_name.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function merges the given DataFrame with a rating DataFrame \n",
      "    based on the anime_id column. It then renames the 'rating_user' \n",
      "    column to 'user_rating' and returns the merged DataFrame.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(merging.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function:\n",
    "- Loading rating df\n",
    "- Añadimos suffixes for ratingdf ya que en los dos df la columna rating tiene el mismo nombre\n",
    "- Cambiamos un par de nombres de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12201, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime = final_df()\n",
    "anime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of source prediction is 0.8886884550084889\n"
     ]
    }
   ],
   "source": [
    "anime_cleaned = clean_anime_df(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merging(anime_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7808397, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that we will call the function name features_user_based_unsupervised from cleaning.py in utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function takes in a merged dataframe, preprocesses the data to drop users \n",
      "    who have not given any ratings and users who have given fewer ratings than a \n",
      "    specified threshold value, and saves the resulting pivot table to a pickle file. \n",
      "    It then compresses the pickle file into a zip file and returns the resulting pivot table.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(features_user_based_unsupervised.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function:\n",
    "-  A user who hasn't given any ratings (-1) has added no value to the engine. So let's drop it.\n",
    "-  Drop rows with NaN values (user has not given any ratings \n",
    "-  There are users who has rated only once. So we should think if we want to consider only users with a minimin ratings as threshold value. Let's say 50.\n",
    "-  Only consider users with at least 200 ratings\n",
    "-  Saving the pivot table to pickle\n",
    "-  Create a zip file for the saved pickle file\n",
    "-  Return the cleaned and filtered features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_user_based_unsupervised(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2378351, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that we will call the function name create_pivot_table_unsupervised from cleaning.py in utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The function create_pivot_table_unsupervised creates a pivot table with rows as anime titles, \n",
      "    columns as user IDs, and the corresponding ratings as values. The pivot table is then saved \n",
      "    to a pickle file and zipped. The function also saves a separate file containing only the \n",
      "    anime titles. Finally, the pivot table is returned.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_pivot_table_unsupervised.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function:\n",
    "- This function takes a DataFrame of features as input, and returns a pivot table of user ratings\n",
    "- Creates the pivot table using pandas' pivot_table method, with user_id as columns, name as index, and user_rating as values\n",
    "- Saves the pivot table as a pickle file using joblib\n",
    "- Compresses the pickle file using zip and saves it\n",
    "- Creates a DataFrame containing the index of the pivot table\n",
    "- Saves the DataFrame as a csv file\n",
    "- Returns the pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = create_pivot_table_unsupervised(features)\n",
    "pivot_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8580, 8551)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation building phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity using KNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a sparse matrix using the csr_matrix function, so we will call the function name matrix_creation_and_training from training.py in utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(matrix_creation_and_training.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function:\n",
    "- Convert pivot table of user-item ratings to a sparse matrix in CSR format\n",
    "- Create k-Nearest Neighbors model with 2 neighbors, Euclidean distance metric, brute force algorithm, and p-norm=2\n",
    "- Fit k-Nearest Neighbors model on the user-item rating matrix\n",
    "- Save the trained k-Nearest Neighbors model to a file using the pickle module\n",
    "- Return the trained k-Nearest Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = matrix_creation_and_training(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;euclidean&#x27;, n_neighbors=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;euclidean&#x27;, n_neighbors=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='euclidean', n_neighbors=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the recommendations we will use the next functions from recommend.py in utils folder:\n",
    "- unsupervised_user_based_recommender\n",
    "- reco\n",
    "- finding_the_closest_title\n",
    "- from_title_to_index\n",
    "- match_the_score\n",
    "- from_index_to_title\n",
    "- create_dict\n",
    "- filtering_and\n",
    "- filtering_or"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(unsupervised_user_based_recommender.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of unsupervised_user_based_recommender:\n",
    "- Load the anime data with features to compare  \n",
    "- Convert the input anime title to lowercase \n",
    "- Load the pivot table to find the index of the input anime title\n",
    "- Find the closest title to the input title based on string similarity\n",
    "- When the user input has no spelling mistakes\n",
    "\t- Print the recommendations for similar animes to the closest title\n",
    "- When the user input has spelling mistakes\n",
    "\t- Print a message asking if the user meant the closest title found\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(reco.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of reco:\n",
    "- Load the trained KNN model for user-based unsupervised learning.\n",
    "- Load the pivot table which stores the user rating data.\n",
    "- Get the index of the anime given the name of the anime.\n",
    "- Get the n nearest neighbors (anime recommendations) of the given anime.\n",
    "- Store the names of the n nearest neighbors in a list.\n",
    "\t- If no recommendations are found, print a message to the user.   \n",
    "- Return the list of recommended anime names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function that takes in a string title and a pandas DataFrame df as input arguments, \n",
      "    and returns a tuple containing the closest matching title to the input title \n",
      "    and the Levenshtein distance score between the closest title and the input title.\n",
      "    in other words, the function returns the most similar title to the name a user typed\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(finding_the_closest_title.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of finding_the_closest_title:\n",
    "- This function takes a string `title` and a pandas DataFrame `df` as input arguments.\n",
    "- Create a new variable `anime` to hold the DataFrame `df` for readability.\n",
    "- Calculate the Levenshtein distance between each title in the 'name' column of the DataFrame and the input `title`.\n",
    "- The `match_the_score` function is used to calculate the distance score.\n",
    "- The `enumerate` function adds an index number to each distance score.\n",
    "- Sort the list of (index, distance score) tuples in descending order by the distance score. sorted_levenshtein_scores = sorted(levenshtein_scores, key=lambda x: x[1], reverse=True)\n",
    "- Get the closest matching title to the input `title` by using the index of the highest scoring match.\n",
    "- The `from_index_to_title` function is used to return the title string from the DataFrame given an index.\n",
    "- Get the Levenshtein distance score of the closest matching title.\n",
    "- Return a tuple containing the closest matching title and its Levenshtein distance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to return the matched index number of the anime name\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(from_title_to_index.__doc__) # just one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to find the closest title, It uses Levenshtein Distance to calculate the differences between sequences\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(match_the_score.__doc__) # just one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to return the anime name that mtches de index number\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(from_index_to_title.__doc__) # just one step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information resulted is pass to:\n",
    "- create_dict\n",
    "- filtering_and\n",
    "- filtering_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The create_dict() function takes in four arguments - names (list of anime names to search for), \n",
      "    gen (list of genres to filter by), typ (list of anime types to filter by), \n",
      "    method (string indicating whether to filter by \"or\" or \"and\"), \n",
      "    and an optional n parameter indicating the maximum number of results to return. \n",
      "    It reads in a pre-processed anime DataFrame, filters it based on the input criteria, \n",
      "    and returns a dictionary of the resulting rows. If there are no matches, \n",
      "    it returns a string indicating it.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_dict.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of create_dict:\n",
    "- This function takes in a list of anime titles `names`, lists of `gen`res and `typ`es, a filtering method `method`, and an optional number of results `n`.\n",
    "- Load the anime dataframe from a CSV file using pandas.\n",
    "- Filter the anime dataframe to only include titles that match those in the input list `names`.\n",
    "- Remove the 'anime_id' and 'members' columns from the resulting dataframe.\n",
    "- Reset the index of the resulting dataframe.\n",
    "- Apply a filtering method based on the input `method`.\n",
    "- If 'or', use the `filtering_or()` function to filter the dataframe.\n",
    "- If 'and', use the `filtering_and()` function to filter the dataframe.\n",
    "- If `method` is neither 'or' nor 'and', raise a ValueError.\n",
    "- Drop any duplicate titles from the resulting dataframe.\n",
    "- Limit the resulting dataframe to the first `n` rows.\n",
    "- If the resulting dataframe is empty, print an error message and return None.\n",
    "- Otherwise, convert the resulting dataframe to a dictionary and return the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function takes a DataFrame df, a list of genres, and a list of types as input arguments. \n",
      "    The function first creates a boolean mask genre_mask by applying a lambda function to \n",
      "    the 'genre' column of the DataFrame. The lambda function checks if the value is a \n",
      "    string using isinstance(x, str) and if all genres in the genres list are present \n",
      "    in the string, which is split by comma and space using x.split(', '). \n",
      "    The all() function returns True if all genres in the genres list are present \n",
      "    in the string. The resulting genre_mask will be True for rows where the genre \n",
      "    column contains all of the genres in the genres list.\n",
      "\n",
      "    Then the function creates another boolean mask type_mask by using the isin() \n",
      "    method to check if each value in the 'type' column of the DataFrame is in the types list.\n",
      "\n",
      "    Finally, the function applies both masks to the DataFrame df using the & operator \n",
      "    to create a new DataFrame filtered_df that includes only rows where both m\n",
      "    asks are True. The function returns the filtered DataFrame.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(filtering_and.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of filtering_and:\n",
    "- This function takes a DataFrame `df`, a list of `genres`, and a list of `types` as input arguments.\n",
    "- Create a boolean mask that filters rows where the genre column contains all of the genres in the `genres` list.\n",
    "- Create a boolean mask that filters rows where the type column is in the `types` list.\n",
    "- Apply both masks to the DataFrame `df` and create a new DataFrame `filtered_df` that includes only rows where both masks are True.\n",
    "- Return the filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The code defines a function \"filtering_or\" that filters a pandas dataframe based on user-defined \n",
      "    genres and types using an \"OR\" method. The function allows the user to select one or all possible \n",
      "    genres and types and returns a filtered dataframe with the selected genres and types. \n",
      "    The function also splits the genre and type columns and explodes them to account for multiple entries.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(filtering_or.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of filtering_or:\n",
    "- Make a copy of the input DataFrame\n",
    "- Split the genre column into a list of genres\n",
    "- Explode the genre column to create a new row for each genre in the list\n",
    "- If genres are specified and 'ALL' is not one of them, filter the DataFrame to keep only rows where the genre is in the specified list  \n",
    "- If types are specified and 'ALL' is not one of them, filter the DataFrame to keep only rows where the type is in the specified list\n",
    "- If both genres and types are specified\n",
    "- If 'ALL' is in the genres list, set genres to be all the unique genres in the filtered DataFrame\n",
    "- If 'ALL' is in the types list, set types to be all the unique types in the filtered DataFrame\n",
    "- Filter the DataFrame to keep only rows where the genre is in the genres list AND the type is in the types list\n",
    "- Return the filtered DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the recommendations for similar animes to \u001b[1mnaruto\u001b[0m \n",
      "\n",
      "or\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'bleach',\n",
       "  'english_title': 'Bleach',\n",
       "  'japanses_title': 'BLEACH - ブリーチ -',\n",
       "  'genre': 'Shounen',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min per ep',\n",
       "  'episodes': 366.0,\n",
       "  'score': 7.9,\n",
       "  'rank': 722.0,\n",
       "  'synopsis': \"Ichigo Kurosaki is an ordinary high schooler—until his family is attacked by a Hollow, a corrupt spirit that seeks to devour human souls. It is then that he meets a Soul Reaper named Rukia Kuchiki, who gets injured while protecting Ichigo's family from the assailant. To save his family, Ichigo accepts Rukia's offer of taking her powers and becomes a Soul Reaper as a result. However, as Rukia is unable to regain her powers, Ichigo is given the daunting task of hunting down the Hollows that plague their town. However, he is not alone in his fight, as he is later joined by his friends—classmates Orihime Inoue, Yasutora Sado, and Uryuu Ishida—who each have their own unique abilities. As Ichigo and his comrades get used to their new duties and support each other on and off the battlefield, the young Soul Reaper soon learns that the Hollows are not the only real threat to the human world. [Written by MAL Rewrite]\",\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/3/40451l.jpg',\n",
       "  'type': 'TV',\n",
       "  'rating': 'PG-13 - Teens 13 or older'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the recommendation as a dictionary\n",
    "# We select the name of the anime we want to find similitudes\n",
    "# Then the number of suggestions we have(we might get less if there not so many o none if there is no matches)\n",
    "# Then the genre we want (or write \"All\" if we shoose \"or\" filter)\n",
    "# Then the type we want (or write \"All\" if we shoose \"or\" filter)\n",
    "# We must select a type or filtering, \"or\"/\"and\" \n",
    "create_dict(unsupervised_user_based_recommender(\"Naruto\",5),[\"Shounen\"],[\"TV\"],\"or\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thebridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16dcd7600f10b4127f6c014f4219dd1a87604b87b18df54c5d893d7c9e33a0ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
