{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised content based recommendation system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os # allows access to OS-dependent functionalities\n",
    "import sys # provides access to system-specific parameters and functions in Python.\n",
    "\n",
    "import numpy as np # functions for working in domain of linear algebra, fourier transform, matrices and arrays\n",
    "import pandas as pd # data analysis and manipulation tool\n",
    "import warnings\n",
    "\n",
    "# setting display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Add the path of the utils directory to sys.path\n",
    "utils_path = os.path.abspath(os.path.join(cwd, '..', 'utils'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "# Utils libraries\n",
    "from cleaning import *\n",
    "from recommend import *\n",
    "from testing import *\n",
    "from training import *\n",
    "\n",
    "#Preparing folder variables\n",
    "main_folder = os.path.abspath(os.path.join(os.pardir))\n",
    "data_folder = (main_folder + \"/\" +\"data\")\n",
    "saved_models_folder = (data_folder + \"/\" + \"saved_models\")\n",
    "raw_data = (data_folder + \"/\" + \"_raw\")\n",
    "processed_data = (data_folder + \"/\" + \"processed\")\n",
    "content_based_supervised_data = (main_folder + \"/\" + \"processed\" + \"/\" + \"content_based_supervised\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preparing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file called \"anime.csv\" from a directory called raw_data and returns the contents as a Pandas DataFrame\n",
    "anime = pd.read_csv(raw_data + \"/\" + \"anime.csv\") \n",
    "\n",
    "# CSV file called \"rating.csv.zip\" from a directory called raw_data and returns the contents as a Pandas DataFrame\n",
    "rating = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of column: rating \n",
      "\n",
      "['R - 17+ (violence & profanity)' 'PG-13 - Teens 13 or older'\n",
      " 'PG - Children' 'R+ - Mild Nudity' 'G - All Ages' 'Rx - Hentai' nan] \n",
      "\n",
      "Unique values of column: genre \n",
      "\n",
      "['Action, Adventure, Comedy, Drama, Sci-Fi, Space'\n",
      " 'Action, Drama, Mystery, Sci-Fi, Space' 'Action, Comedy, Sci-Fi' ...\n",
      " 'Action, Fantasy, Super Power, Supernatural, Vampire'\n",
      " 'Action, Adventure, Fantasy, Game, Kids'\n",
      " 'Comedy, Fantasy, Slice of Life, Supernatural'] \n",
      "\n",
      "Unique values of column: type \n",
      "\n",
      "['TV' 'Movie' 'OVA' 'Special' 'ONA' 'Music' nan] \n",
      "\n",
      "Unique values of column: source \n",
      "\n",
      "['Original' 'Manga' 'Light novel' 'Game' 'Visual novel' '4-koma manga'\n",
      " 'Novel' 'Other' 'Unknown' 'Picture book' 'Web manga' 'Music' 'Radio'\n",
      " 'Book' 'Card game' 'Mixed media'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for unique values.\n",
    "to_check = [\"rating\",\"genre\",\"type\",\"source\"]\n",
    "for i in to_check:\n",
    "    print (\"Unique values of column:\",i,\"\\n\")\n",
    "    print (anime[i].unique(),\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have some NaN and Unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score             14.048029\n",
      "rank               9.572986\n",
      "synopsis           2.475207\n",
      "rating             1.614622\n",
      "genre              0.499959\n",
      "type               0.172117\n",
      "japanses_title     0.155725\n",
      "anime_id           0.000000\n",
      "name               0.000000\n",
      "english_title      0.000000\n",
      "source             0.000000\n",
      "duration           0.000000\n",
      "episodes           0.000000\n",
      "members            0.000000\n",
      "cover              0.000000\n",
      "dtype: float64\n",
      "Total number of records: 12201\n"
     ]
    }
   ],
   "source": [
    "# the null values\n",
    "print(((anime.isnull().sum() / len(anime))*100).sort_values(ascending = False))\n",
    "print(f\"Total number of records: {len(anime)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns rank, synopsis and japanses_title will only be use for showing the results of the recommendation, so we do not care about this 3 columns.\n",
    "\n",
    "We will deal with the others.\n",
    "\n",
    "To do that we will call the next functions from cleaning.py in utils folder.\n",
    "- clean_anime_df\n",
    "- predict_source\n",
    "- clean_synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function clean_anime_df() takes an anime dataframe as input and performs several \n",
      "    cleaning and preprocessing steps, such as removing special characters from anime names, \n",
      "    converting all names to lowercase, filling missing values for \"episodes\" and \"score\" \n",
      "    columns with their median, dropping rows with null values for \"genre\" or \"type\" columns, \n",
      "    and saving the cleaned dataframe to a CSV file. The cleaned dataframe is also returned as output.\n"
     ]
    }
   ],
   "source": [
    "print(clean_anime_df.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The function takes a pandas dataframe containing anime data and \n",
      "    fills in missing values in the 'source' column using a Decision \n",
      "    Tree Classifier based on the 'episodes' and 'type' columns. The \n",
      "    'type' column is converted to categorical data using get_dummies \n",
      "    before fitting the model. The function returns the original \n",
      "    dataframe with missing values filled in and the model accuracy score.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(predict_source.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This code uses regular expressions to clean up the text in the \"synopsis\" column\n",
      "    of a pandas DataFrame. It removes any text in square brackets, removes any c\n",
      "    arriage returns or newline characters, and removes any extra whitespace at \n",
      "    the beginning or end of the string.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(clean_synopsis.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function:\n",
    "- Create a copy of the original dataframe called anime_cleaned\n",
    "- Remove all non-word characters from the name column and replace them with spaces\n",
    "- Convert all names to lowercase\n",
    "- Replace all \"Unknown\" values in the episodes column with NaN\n",
    "- Replace all NaN values in the episodes column with the median of the column\n",
    "- Convert the score column to float type\n",
    "- Replace all NaN values in the score column with the median of the column\n",
    "- Convert the members column to float type\n",
    "- Apply the clean_synopsis function to the synopsis column\n",
    "    - Remove \\r and \\n from synopsis\n",
    "    - Remove extra spaces from synopsis\n",
    "    - Replace encoded characters\n",
    "    - Return synopsis\n",
    "- Add prediction to the source column of the dataframe using the predict_source function\n",
    "    - change unknown values to NaN from 'source' column\n",
    "    - fill missing values in the 'episodes' column with 0\n",
    "    - create dummy variables for the 'type' column\n",
    "    - create dummy variables for the 'rating' column\n",
    "    - First, we area going to split the genre column by comma, then expand the list, so there is a column for each genre. We will have 13 columns, because the anime with most genres tags has 13 tags\n",
    "    - Now we can get the list of unique genres. We \"convert\" the dataframe into a single dimension array and take the unique values\n",
    "    - Getting the dummy variables will result in having a lot more columns than unique genres\n",
    "    - So we sum up the columns with the same genre to have a single column for each genre\n",
    "    - split the data into training and validation sets\n",
    "    - create the decision tree classifier\n",
    "    - train the model using the training data\n",
    "    - predict the 'source' values for the validation data\n",
    "    - fill the 'NaN' 'source' values in the original DataFrame with the predicted values\n",
    "    - undo the get_dummies() operation to convert the one-hot encoded 'type' and 'rating' columns back to a single categorical column\n",
    "    - Dropping unnecessary columns\n",
    "    - calculate the accuracy of the model\n",
    "- Replace all NaN values in the genre column with the mode of the column\n",
    "- Replace all NaN values in the rating column with the mode of the column\n",
    "- Replace all NaN values in the type column with the mode of the column\n",
    "- Save the cleaned dataframe to a CSV file called \"_anime_to_compare_with_name.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of source prediction is 0.8886884550084889\n"
     ]
    }
   ],
   "source": [
    "anime_cleaned = clean_anime_df(anime)# from cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12201, 15)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_cleaned.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have no null values in the columns we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank              9.572986\n",
      "synopsis          2.475207\n",
      "japanses_title    0.155725\n",
      "anime_id          0.000000\n",
      "name              0.000000\n",
      "english_title     0.000000\n",
      "genre             0.000000\n",
      "source            0.000000\n",
      "duration          0.000000\n",
      "episodes          0.000000\n",
      "score             0.000000\n",
      "members           0.000000\n",
      "cover             0.000000\n",
      "type              0.000000\n",
      "rating            0.000000\n",
      "dtype: float64\n",
      "Total number of records: 12201\n"
     ]
    }
   ],
   "source": [
    "# let's check the result of this cleaning process.\n",
    "print(((anime_cleaned.isnull().sum() / len(anime_cleaned))*100).sort_values(ascending = False))\n",
    "print(f\"Total number of records: {len(anime_cleaned)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to prepare the data for NearestNeighbors model\n",
    "\n",
    "To do that we will call the next function from cleaning.py in utils folder.\n",
    "- prepare_supervised_content_based\n",
    "\n",
    "Since que are the columnas \"episodes\", \"score\" and \"members\", we will as well use:\n",
    "\n",
    "- MinMaxScaler and calling fit_transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function prepares the content-based features for a supervised \n",
      "    learning model. It first splits the genres into separate columns and \n",
      "    gets unique genres. It then creates dummy variables for genres and \n",
      "    type, and sum up the columns for the same genre to have a single \n",
      "    column for each genre. Finally, it drops irrelevant columns and saves \n",
      "    the resulting dataframe to a CSV file. The function returns the resulting dataframe.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prepare_supervised_content_based.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of this function:\n",
    "- Split the \"genre\" column into multiple columns  \n",
    "- Get unique genre values \n",
    "- Create dummy variables for the genre columns  \n",
    "- For each unique genre, sum up the corresponding dummy variables and add a new column for that genre  \n",
    "- Create dummy variables for the \"type\" column  \n",
    "- Create dummy variables for the \"rating\" column \n",
    "- Create dummy variables for the \"source\" column   \n",
    "- Concatenate the \"type\" dummy variables with the existing DataFrame  \n",
    "- Drop unnecessary columns  \n",
    "- Create a new DataFrame that is a copy of the modified DataFrame\n",
    "- Reset the index of the new DataFrame\n",
    "- Save the modified DataFrame as a CSV file\n",
    "- Return the modified DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_features = prepare_supervised_content_based(anime_cleaned) # from cleaning.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anime_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12201, 73)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_features.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the MinMaxScaler to a dataset, typically we need to first \"fit\" the scaler to the data, which means calculating the minimum and maximum values for each feature in the dataset. This is done using the fit() method of the MinMaxScaler object. After fitting the scaler, you can then \"transform\" the data using the transform() method, which applies the scaling formula to each feature.\n",
    "\n",
    "However, in some cases, if better to both fit the scaler to the data and transform the data in a single step. This is where the fit_transform() method comes in handy. Calling fit_transform() on a MinMaxScaler object will both fit the scaler to the data and transform the data in a single step.\n",
    "\n",
    "Using a MinMaxScaler and calling fit_transform() is a common way to scale feature values to be within a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "min_max_features = min_max.fit_transform(anime_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12201, 73)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.37589433e-02, 9.50413223e-01, 4.80139302e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 8.99449036e-01, 1.35742550e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.37589433e-02, 8.77410468e-01, 2.79180047e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [5.50357733e-04, 6.43250689e-01, 3.70840862e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [5.50357733e-04, 6.30853994e-01, 1.01586725e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 6.33608815e-01, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rounding the values using np.round(min_max_features, 2) will round each value in min_max_features to 2 decimal places, which can make the resulting array easier to read and work with. This is especially useful when dealing with large arrays or matrices, where the values can be difficult to interpret if there are too many decimal places.\n",
    "\n",
    "It's worth noting that the use of np.round() in this code is not strictly necessary and is largely a matter of personal preference. Some people might prefer to work with the original unscaled values or may choose to use a different rounding method depending on the specific requirements of their project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.95, 0.48, ..., 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.9 , 0.14, ..., 0.  , 0.  , 0.  ],\n",
       "       [0.01, 0.88, 0.28, ..., 0.  , 0.  , 1.  ],\n",
       "       ...,\n",
       "       [0.  , 0.64, 0.  , ..., 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.63, 0.  , ..., 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.63, 0.  , ..., 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(min_max_features,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best parameters for NearestNeighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The function param_NearestNeighbors uses GridSearchCV from scikit-learn \n",
      "    to perform a grid search over a range of hyperparameters for the \n",
      "    NearestNeighbors model. It takes a dataframe df as input and returns \n",
      "    the best hyperparameters found during the grid search. The hyperparameters \n",
      "    being searched over include n_neighbors, radius, algorithm, leaf_size, \n",
      "    metric, and p. The scoring metric being used is \"accuracy\" and the refit \n",
      "    parameter is set to \"precision_score\". cv=2 sets the number of cross-validation \n",
      "    folds to 2, and n_jobs=-1 sets the number of CPU cores used to parallelize \n",
      "    the search to be the maximum available.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(param_NearestNeighbors.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV function works by training and evaluating the estimator (in this case, the NearestNeighbors() object) for each combination of parameter values specified in the param_grid argument. It then returns the combination of parameter values that resulted in the best performance, as determined by the specified scoring metric.\n",
    "\n",
    "Overall, using GridSearchCV in NearestNeighbors() allows us to fine-tune the hyperparameters of the k-nearest neighbors algorithm to improve its accuracy and make better recommendations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finde the best parameters passing the min_max_features.\n",
    "\n",
    "Steps:\n",
    "- Define dictionary of hyperparameters to test using GridSearchCV\n",
    "- Create GridSearchCV object with NearestNeighbors algorithm and hyperparameters defined in the parametros dictionary\n",
    "- Return the best hyperparameters found by the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_NearestNeighbors(min_max_features) # from testing.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The function model_NearestNeighbors builds and trains a \n",
      "    k-Nearest Neighbors model on a given dataset, using specified \n",
      "    parameters. It then saves the indices of the nearest neighbors \n",
      "    to a file and returns them.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(model_NearestNeighbors.__doc__) # from testing.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build and \"train\" the model using NearestNeighbors algorithm\n",
    "\t- algorithm: algorithm used to compute the nearest neighbors (‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’)\n",
    "\t- leaf_size: leaf size passed to BallTree or KDTree\n",
    "\t- metric: distance metric used for the tree. Can be 'minkowski', 'euclidean', etc.\n",
    "\t- n_neighbors: number of neighbors to use for kneighbors queries\n",
    "\t- p: power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance\n",
    "- Get the distances and indices of the nearest neighbors\n",
    "\t- distances: array representing the lengths to points, only present if return_distance=True\n",
    "\t- indices: indices of the nearest points in the population matrix\n",
    "- Save the model to a file using joblib.dump\n",
    "- Return the indices of the nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,  3445, 10833, ...,   533,  2205,  1041],\n",
       "       [    1,  7697,  4530, ...,  9351,   992,  7474],\n",
       "       [    2,  3409, 11218, ...,  3136,  1181,  2198],\n",
       "       ...,\n",
       "       [12198,  7973,  5977, ...,  1850,  8626,  8464],\n",
       "       [12199,  1825,   187, ...,  2339,  4352,  4780],\n",
       "       [12200,  8769,  9684, ...,  4427,  8354,  2523]], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NearestNeighbors(min_max_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the recommendations we will use the next functions from recommend.py in utils folder:\n",
    "- print_similar_animes\n",
    "- finding_the_closest_title\n",
    "- from_title_to_index\n",
    "- match_the_score\n",
    "- from_index_to_title\n",
    "- create_dict\n",
    "- filtering_and\n",
    "- filtering_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function takes a user input anime name query and returns a list of recommended anime similar to the query.\n",
      "    It uses a pre-trained model and a dataset of anime information to find recommendations. \n",
      "    If the user query has any misspelling, the function tries to find the closest match to \n",
      "    the query and provides recommendations based on that.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(print_similar_animes.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of print_similar_animes:\n",
    "- Load pre-trained k-Nearest Neighbors model from file\n",
    "- Load anime data from CSV file\n",
    "- Find the closest title in the anime dataset to the user's query\n",
    "- If the distance score is 100, the user's query is an exact match for a title in the dataset\n",
    "\t- Get the index of the exact match in the dataset \n",
    "\t- Get the indices of the k-nearest neighbors of the exact match\n",
    "\t- Remove the index of the exact match from the array of neighbors\n",
    "\t- For each neighbor index, get the name of the anime and add it to the list of recommendations\n",
    "\t- Return the list of recommendations\n",
    "- If the distance score is not 100, the user's query is a misspelling or a partial match  \n",
    "\t- Ask the user if they meant the closest title found in the dataset\n",
    "\t- Get the index of the closest title in the dataset\n",
    "\t- Get the indices of the k-nearest neighbors of the closest title\n",
    "\t- Remove the index of the closest title from the array of neighbors\n",
    "\t- For each neighbor index, get the name of the anime and add it to the list of recommendations       \n",
    "\t- Return the list of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function that takes in a string title and a pandas DataFrame df as input arguments, \n",
      "    and returns a tuple containing the closest matching title to the input title \n",
      "    and the Levenshtein distance score between the closest title and the input title.\n",
      "    in other words, the function returns the most similar title to the name a user typed\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(finding_the_closest_title.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of finding_the_closest_title:\n",
    "- This function takes a string `title` and a pandas DataFrame `df` as input arguments.\n",
    "- Create a new variable `anime` to hold the DataFrame `df` for readability.\n",
    "- Calculate the Levenshtein distance between each title in the 'name' column of the DataFrame and the input `title`.\n",
    "- The `match_the_score` function is used to calculate the distance score.\n",
    "- The `enumerate` function adds an index number to each distance score.\n",
    "- Sort the list of (index, distance score) tuples in descending order by the distance score. sorted_levenshtein_scores = sorted(levenshtein_scores, key=lambda x: x[1], reverse=True)\n",
    "- Get the closest matching title to the input `title` by using the index of the highest scoring match.\n",
    "- The `from_index_to_title` function is used to return the title string from the DataFrame given an index.\n",
    "- Get the Levenshtein distance score of the closest matching title.\n",
    "- Return a tuple containing the closest matching title and its Levenshtein distance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to return the matched index number of the anime name\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(from_title_to_index.__doc__) # just one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to find the closest title, It uses Levenshtein Distance to calculate the differences between sequences\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(match_the_score.__doc__) # just one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function to return the anime name that mtches de index number\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(from_index_to_title.__doc__) # just one step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information resulted is pass to:\n",
    "- create_dict\n",
    "- filtering_and\n",
    "- filtering_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The create_dict() function takes in four arguments - names (list of anime names to search for), \n",
      "    gen (list of genres to filter by), typ (list of anime types to filter by), \n",
      "    method (string indicating whether to filter by \"or\" or \"and\"), \n",
      "    and an optional n parameter indicating the maximum number of results to return. \n",
      "    It reads in a pre-processed anime DataFrame, filters it based on the input criteria, \n",
      "    and returns a dictionary of the resulting rows. If there are no matches, \n",
      "    it returns a string indicating it.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_dict.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of create_dict:\n",
    "- This function takes in a list of anime titles `names`, lists of `gen`res and `typ`es, a filtering method `method`, and an optional number of results `n`.\n",
    "- Load the anime dataframe from a CSV file using pandas.\n",
    "- Filter the anime dataframe to only include titles that match those in the input list `names`.\n",
    "- Remove the 'anime_id' and 'members' columns from the resulting dataframe.\n",
    "- Reset the index of the resulting dataframe.\n",
    "- Apply a filtering method based on the input `method`.\n",
    "- If 'or', use the `filtering_or()` function to filter the dataframe.\n",
    "- If 'and', use the `filtering_and()` function to filter the dataframe.\n",
    "- If `method` is neither 'or' nor 'and', raise a ValueError.\n",
    "- Drop any duplicate titles from the resulting dataframe.\n",
    "- Limit the resulting dataframe to the first `n` rows.\n",
    "- If the resulting dataframe is empty, print an error message and return None.\n",
    "- Otherwise, convert the resulting dataframe to a dictionary and return the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function takes a DataFrame df, a list of genres, and a list of types as input arguments. \n",
      "    The function first creates a boolean mask genre_mask by applying a lambda function to \n",
      "    the 'genre' column of the DataFrame. The lambda function checks if the value is a \n",
      "    string using isinstance(x, str) and if all genres in the genres list are present \n",
      "    in the string, which is split by comma and space using x.split(', '). \n",
      "    The all() function returns True if all genres in the genres list are present \n",
      "    in the string. The resulting genre_mask will be True for rows where the genre \n",
      "    column contains all of the genres in the genres list.\n",
      "\n",
      "    Then the function creates another boolean mask type_mask by using the isin() \n",
      "    method to check if each value in the 'type' column of the DataFrame is in the types list.\n",
      "\n",
      "    Finally, the function applies both masks to the DataFrame df using the & operator \n",
      "    to create a new DataFrame filtered_df that includes only rows where both m\n",
      "    asks are True. The function returns the filtered DataFrame.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(filtering_and.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of filtering_and:\n",
    "- This function takes a DataFrame `df`, a list of `genres`, and a list of `types` as input arguments.\n",
    "- Create a boolean mask that filters rows where the genre column contains all of the genres in the `genres` list.\n",
    "- Create a boolean mask that filters rows where the type column is in the `types` list.\n",
    "- Apply both masks to the DataFrame `df` and create a new DataFrame `filtered_df` that includes only rows where both masks are True.\n",
    "- Return the filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The code defines a function \"filtering_or\" that filters a pandas dataframe based on user-defined \n",
      "    genres and types using an \"OR\" method. The function allows the user to select one or all possible \n",
      "    genres and types and returns a filtered dataframe with the selected genres and types. \n",
      "    The function also splits the genre and type columns and explodes them to account for multiple entries.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(filtering_or.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of filtering_or:\n",
    "- Make a copy of the input DataFrame\n",
    "- Split the genre column into a list of genres\n",
    "- Explode the genre column to create a new row for each genre in the list\n",
    "- If genres are specified and 'ALL' is not one of them, filter the DataFrame to keep only rows where the genre is in the specified list  \n",
    "- If types are specified and 'ALL' is not one of them, filter the DataFrame to keep only rows where the type is in the specified list\n",
    "- If both genres and types are specified\n",
    "- If 'ALL' is in the genres list, set genres to be all the unique genres in the filtered DataFrame\n",
    "- If 'ALL' is in the types list, set types to be all the unique types in the filtered DataFrame\n",
    "- Filter the DataFrame to keep only rows where the genre is in the genres list AND the type is in the types list\n",
    "- Return the filtered DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I guess you misspelled the name\n",
      " Are you looking similitudes for the anime named \u001b[1mnaruto\u001b[0m? \n",
      "Here are the recommendations:\n",
      "or\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'eyeshield 21',\n",
       "  'english_title': 'Eyeshield 21',\n",
       "  'japanses_title': 'アイシールド21',\n",
       "  'genre': 'Shounen',\n",
       "  'source': 'Manga',\n",
       "  'duration': '23 min per ep',\n",
       "  'episodes': 145.0,\n",
       "  'score': 7.92,\n",
       "  'rank': 702.0,\n",
       "  'synopsis': 'Shy, reserved, and small-statured, Deimon High School student Sena Kobayakawa is the perfect target for bullies. However, as a result of running errands throughout his life, Sena has become agile and developed a skill for avoiding crowds of people. After the cunning Youichi Hiruma—captain of the Deimon Devil Bats football team—witnesses Sena\\'s rapid legs in motion, he coerces the timid boy into joining his squad. As Hiruma wants to conceal Sena\\'s identity from other clubs, Sena is forced to hide under the visored helmet of \"Eyeshield 21,\" a mysterious running back wearing the number 21 jersey. The legendary Eyeshield 21 can supposedly run at the speed of light and has achieved remarkable feats in the United States during his time at the Notre Dame College. Accustomed to avoiding his problems in the past, Sena\\'s specialty might just help him become the new secret weapon of the Deimon Devil Bats. As he interacts with his teammates, Sena gradually gains more self-confidence and forges valuable bonds along the way. [Written by MAL Rewrite]',\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/12/66961l.jpg',\n",
       "  'type': 'TV',\n",
       "  'rating': 'PG-13 - Teens 13 or older'},\n",
       " {'name': 'one piece',\n",
       "  'english_title': 'One Piece',\n",
       "  'japanses_title': 'ONE PIECE',\n",
       "  'genre': 'Shounen',\n",
       "  'source': 'Manga',\n",
       "  'duration': '24 min',\n",
       "  'episodes': 2.0,\n",
       "  'score': 8.68,\n",
       "  'rank': 57.0,\n",
       "  'synopsis': 'Gol D. Roger was known as the \"Pirate King,\" the strongest and most infamous being to have sailed the Grand Line. The capture and execution of Roger by the World Government brought a change throughout the world. His last words before his death revealed the existence of the greatest treasure in the world, One Piece. It was this revelation that brought about the Grand Age of Pirates, men who dreamed of finding One Piece—which promises an unlimited amount of riches and fame—and quite possibly the pinnacle of glory and the title of the Pirate King. Enter Monkey D. Luffy, a 17-year-old boy who defies your standard definition of a pirate. Rather than the popular persona of a wicked, hardened, toothless pirate ransacking villages for fun, Luffy\\'s reason for being a pirate is one of pure wonder: the thought of an exciting adventure that leads him to intriguing people and ultimately, the promised treasure. Following in the footsteps of his childhood hero, Luffy and his crew travel across the Grand Line, experiencing crazy adventures, unveiling dark mysteries and battling strong enemies, all in order to reach the most coveted of all fortunes—One Piece. [Written by MAL Rewrite]',\n",
       "  'cover': 'https://cdn.myanimelist.net/images/anime/6/73245l.jpg',\n",
       "  'type': 'TV',\n",
       "  'rating': 'PG-13 - Teens 13 or older'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the recommendation as a dictionary\n",
    "# We selec the name of the anime we want to find similitudes\n",
    "# Then the genre we want\n",
    "# Then the type we want (or write \"All\" if we shoose \"or\" filter)\n",
    "# We must select a type or filtering, \"or\"/\"and\" \n",
    "# Then the number of suggestions we have(we might get less if there not so many o none if there is no matches)\n",
    "\n",
    "create_dict(print_similar_animes(\"Naruto\"),[\"Shounen\"],[\"TV\"],\"or\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info():\n",
    "    # Import Libraries\n",
    "    import requests, json, os, sys, time\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    '''\n",
    "    Preparing folder variables.\n",
    "    '''\n",
    "    os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "    main_folder = sys.path[0]\n",
    "    data_folder = (main_folder + \"\\data\")\n",
    "\n",
    "    '''\n",
    "    Creating time variables.\n",
    "    '''\n",
    "    current_time = time.strftime(\"%H_%M_%S\",time.localtime())\n",
    "    date = datetime.now()\n",
    "    actual_date = date.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "\n",
    "    #Remove the limit to see the df\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    #Creating the necessary lists\n",
    "    anime_list = []\n",
    "\n",
    "    '''\n",
    "    To check if there is an empty value. If the category is empty, it returns None.\n",
    "    '''\n",
    "    def try_it(i):\n",
    "        try:\n",
    "            return i[\"name\"]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    '''\n",
    "    Def with a try check to get the finishing time of an anime, in case the anime is a movie, then it returns the release time.\n",
    "    If the anime is not a movie, it checks for the finishing time. If there is no finishing time, it returns None \n",
    "    '''\n",
    "\n",
    "    url = \"https://api.jikan.moe/v4/anime\" # url of the api\n",
    "\n",
    "    r = requests.get(url)# request to a web page (url)    \n",
    "    \n",
    "    data = r.json() # creating a variable for all the info we get\n",
    "\n",
    "    n_pages = data['pagination']['last_visible_page']\n",
    "    \n",
    "    for page in range (1,n_pages +1):\n",
    "        r_page = requests.get(url + '?page=' + str(page)) # request to a web page (url)\n",
    "        content = r_page.json()\n",
    "        print (page)\n",
    "        data = content[\"data\"]\n",
    "        time.sleep(1)\n",
    "        for char in data: \n",
    "            \n",
    "            try: # First try yo check if the page exist or not\n",
    "                # Creation of the necessary dictionary o store the values in each loop # We specify which information to get in each Item\n",
    "                anime_dict = {\"Anime_id\" : char[\"mal_id\"] if char[\"mal_id\"]  else None,\n",
    "                            \"Cover\" : char[\"images\"][\"jpg\"][\"large_image_url\"] if char[\"images\"][\"jpg\"][\"large_image_url\"]  else None,\n",
    "                            \"English_Title\" : char[\"title\"] if char[\"title\"]  else None,\n",
    "                            \"Japanses_Title\" : char[\"title_japanese\"] if char[\"title_japanese\"]  else None,\n",
    "                            \"Type\" : char[\"type\"] if char[\"type\"]  else None,\n",
    "                            \"Source\" : char[\"source\"] if char[\"status\"] else None,\n",
    "                            \"Audience\" : [try_it(i) for i in char[\"demographics\"]], # List comprehension calling the Def try_it\n",
    "                            \"N_Episodes\" : (int(char[\"episodes\"])) if char[\"episodes\"] else 0,\n",
    "                            \"Duration\" : char[\"duration\"] if char[\"duration\"]  else None,\n",
    "                            \"Rating\" : char[\"rating\"] if char[\"rating\"] else None,\n",
    "                            \"Score\" : char[\"score\"] if char[\"score\"]  else None,\n",
    "                            \"Scored_by\" : char[\"scored_by\"] if char[\"scored_by\"]  else None,\n",
    "                            \"Rank\" : (int(char[\"rank\"])) if char[\"rank\"] else None,\n",
    "                            \"Season\" : char[\"season\"] if char[\"season\"] else None,\n",
    "                            \"Genre\" : [try_it(i) for i in char[\"genres\"]],# List comprehension calling the Def try_it\n",
    "                            \"Theme\" : [try_it(i) for i in char[\"themes\"]],# List comprehension calling the Def try_it\n",
    "                            \"Released\" : (int(char[\"aired\"][\"prop\"][\"from\"][\"year\"])) if char[\"aired\"][\"prop\"][\"from\"][\"year\"] else None, # If else in one line\n",
    "                            \"Studios\" : [try_it(i) for i in char[\"studios\"]],# List comprehension calling the Def try_it\n",
    "                            \"Producers\" : [try_it(i) for i in char[\"producers\"]],# List comprehension calling the Def try_it\n",
    "                            \"Synopsis\" : char[\"synopsis\"] if char[\"synopsis\"]  else None,\n",
    "                            }\n",
    "                            \n",
    "                anime_list.append(anime_dict) # Append the loop info to anime_list\n",
    "                \n",
    "            # Ending of the first try specifying the error\n",
    "            except:\n",
    "                if r_page.status_code == 429: #If there is a 429 error we show it on screen and tell us the respuesta.reason\n",
    "                    print (f\"El código de estado de la petición es: {r_page.status_code}. Estatus {r_page.reason}. No se puede recoger información de la página {id}\\n\")\n",
    "                else:\n",
    "                    #If there is a any other error we show it on screen and tell us the respuesta.reason\n",
    "                    print (f\"El código de estado de la petición es: {r_page.status_code}. Estatus {r_page.reason}. No se puede recoger información de la página {id}\\n\")\n",
    "                continue\n",
    "\n",
    "    # We create df from anime_list and save it in a csv file adding actual date and time variables to the name\n",
    "    anime_df = pd.DataFrame(anime_list)\n",
    "    anime_csv = os.path.join(\"anime_\" + actual_date+ \"_\" +current_time + \".csv\")# Saving the image to the images folder\n",
    "    anime_df.to_csv(anime_csv, sep = ';', index = False)\n",
    "    print(f'anime_{actual_date}{current_time}.csv created\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "anime_2023_02_2802_16_12.csv created\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thebridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16dcd7600f10b4127f6c014f4219dd1a87604b87b18df54c5d893d7c9e33a0ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
