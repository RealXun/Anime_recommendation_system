{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification setting, the task is to predict which genre or categories a movie belongs to based on certain features such as plot, actors, directors, etc. This type of recommendation is known as content-based filtering.\n",
    "\n",
    "In a regression setting, the task is to predict the rating a user would give to a movie based on their past ratings and demographic information. This type of recommendation is known as collaborative filtering.\n",
    "\n",
    "The choice between classification and regression largely depends on the data and the problem being addressed. Both techniques have their own advantages and disadvantages, and it is up to the data scientist to choose the appropriate model based on their understanding of the data and the problem.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD-based anime recommendation system using Truncated SVD with PCA, \n",
    "perform hyperparameter tuning with GridSearchCV to find the best parameters for the model, evaluate the model using mean squared error, \n",
    "and predict N number of animes for each user. The predictions will be saved in a dataframe sorted from higher to lower and saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "sounds_folder = (main_folder + \"\\sounds\")\n",
    "saved_models = (main_folder + \"\\saved_models\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "user_based_unsupervised_data = (data_folder + \"\\processed\\_user_based_unsupervised\")\n",
    "content_based_unsupervised_data = (data_folder + \"\\processed\\content_based_unsupervised\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime ratings dataset into a pandas dataframe\n",
    "df = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "# Merge the ratings dataframe with the anime names dataframe\n",
    "df = pd.merge(df, pd.read_csv(raw_data + \"/\" + \"anime.csv\"), on='anime_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducin the df using a sample to test the model faster to see if it works.\n",
    "size = 100000\n",
    "rating_sample = df.groupby(\"rating_x\", group_keys=False).apply(lambda x: x.sample(int(np.rint(size*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix from the ratings dataframe\n",
    "matrix = df.pivot_table(index='user_id', columns='name', values='rating_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the mean rating for each anime\n",
    "matrix = matrix.fillna(matrix.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\christiandda\\\\Documents\\\\GitHub\\\\Anime_recommendation_systems-1\\\\src\\\\data\\\\processed/tests_matrix.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the matrix to pickle\n",
    "import joblib\n",
    "import pickle\n",
    "joblib.dump(matrix,processed_data + \"/\" + \"tests_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Truncated SVD with PCA\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "anime_matrix = svd.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(anime_matrix, matrix, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 220, in __call__\n",
      "    return self._score(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 262, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 72, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'TruncatedSVD' object has no attribute 'predict'\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 220, in __call__\n",
      "    return self._score(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 262, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 72, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'TruncatedSVD' object has no attribute 'predict'\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 220, in __call__\n",
      "    return self._score(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 262, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 72, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'TruncatedSVD' object has no attribute 'predict'\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 220, in __call__\n",
      "    return self._score(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 262, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 72, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'TruncatedSVD' object has no attribute 'predict'\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 220, in __call__\n",
      "    return self._score(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 262, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 72, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'TruncatedSVD' object has no attribute 'predict'\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "10 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 204, in fit\n",
      "    self.fit_transform(X)\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 237, in fit_transform\n",
      "    raise ValueError(\n",
      "ValueError: n_components(100) must be <= n_features(50).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 204, in fit\n",
      "    self.fit_transform(X)\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\", line 237, in fit_transform\n",
      "    raise ValueError(\n",
      "ValueError: n_components(200) must be <= n_features(50).\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=TruncatedSVD(),\n",
       "             param_grid={&#x27;n_components&#x27;: [50, 100, 200], &#x27;random_state&#x27;: [42]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=TruncatedSVD(),\n",
       "             param_grid={&#x27;n_components&#x27;: [50, 100, 200], &#x27;random_state&#x27;: [42]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=TruncatedSVD(),\n",
       "             param_grid={'n_components': [50, 100, 200], 'random_state': [42]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform hyperparameter tuning with GridSearchCV\n",
    "param_grid = {'n_components': [50, 100, 200], 'random_state': [42]}\n",
    "grid_search = GridSearchCV(TruncatedSVD(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TruncatedSVD' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Evaluate the best model on the test set\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_model \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_estimator_\n\u001b[1;32m----> 3\u001b[0m y_pred \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39;49mpredict(y_test)\n\u001b[0;32m      4\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TruncatedSVD' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TruncatedSVD' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Predict N number of animes for each user\u001b[39;00m\n\u001b[0;32m      2\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> 3\u001b[0m user_predictions \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39;49mpredict(anime_matrix)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TruncatedSVD' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Predict N number of animes for each user\n",
    "N = 10\n",
    "user_predictions = best_model.predict(anime_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of anime names and predicted ratings\n",
    "anime_names = matrix.columns\n",
    "prediction_df = pd.DataFrame(user_predictions, columns=anime_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the predictions for a specific anime\n",
    "input_anime = input(\"Enter an anime name: \")\n",
    "sorted_predictions = prediction_df[input_anime].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top N recommendations\n",
    "print(\"Top\", N, \"recommendations for\", input_anime, \":\")\n",
    "for i in range(N):\n",
    "    print(sorted_predictions.index[i], sorted_predictions.values[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "sounds_folder = (main_folder + \"\\sounds\")\n",
    "saved_models = (main_folder + \"\\saved_models\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "user_based_unsupervised_data = (data_folder + \"\\processed\\_user_based_unsupervised\")\n",
    "content_based_unsupervised_data = (data_folder + \"\\processed\\content_based_unsupervised\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime ratings dataset into a pandas dataframe\n",
    "df = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "# Merge the ratings dataframe with the anime names dataframe\n",
    "df = pd.merge(df, pd.read_csv(raw_data + \"/\" + \"anime.csv\"), on='anime_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating_x</th>\n",
       "      <th>name</th>\n",
       "      <th>genre</th>\n",
       "      <th>type</th>\n",
       "      <th>episodes</th>\n",
       "      <th>rating_y</th>\n",
       "      <th>members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>Naruto</td>\n",
       "      <td>Action, Comedy, Martial Arts, Shounen, Super P...</td>\n",
       "      <td>TV</td>\n",
       "      <td>220</td>\n",
       "      <td>7.81</td>\n",
       "      <td>683297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>Naruto</td>\n",
       "      <td>Action, Comedy, Martial Arts, Shounen, Super P...</td>\n",
       "      <td>TV</td>\n",
       "      <td>220</td>\n",
       "      <td>7.81</td>\n",
       "      <td>683297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>Naruto</td>\n",
       "      <td>Action, Comedy, Martial Arts, Shounen, Super P...</td>\n",
       "      <td>TV</td>\n",
       "      <td>220</td>\n",
       "      <td>7.81</td>\n",
       "      <td>683297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>Naruto</td>\n",
       "      <td>Action, Comedy, Martial Arts, Shounen, Super P...</td>\n",
       "      <td>TV</td>\n",
       "      <td>220</td>\n",
       "      <td>7.81</td>\n",
       "      <td>683297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>Naruto</td>\n",
       "      <td>Action, Comedy, Martial Arts, Shounen, Super P...</td>\n",
       "      <td>TV</td>\n",
       "      <td>220</td>\n",
       "      <td>7.81</td>\n",
       "      <td>683297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  anime_id  rating_x    name  \\\n",
       "0        1        20        -1  Naruto   \n",
       "1        3        20         8  Naruto   \n",
       "2        5        20         6  Naruto   \n",
       "3        6        20        -1  Naruto   \n",
       "4       10        20        -1  Naruto   \n",
       "\n",
       "                                               genre type episodes  rating_y  \\\n",
       "0  Action, Comedy, Martial Arts, Shounen, Super P...   TV      220      7.81   \n",
       "1  Action, Comedy, Martial Arts, Shounen, Super P...   TV      220      7.81   \n",
       "2  Action, Comedy, Martial Arts, Shounen, Super P...   TV      220      7.81   \n",
       "3  Action, Comedy, Martial Arts, Shounen, Super P...   TV      220      7.81   \n",
       "4  Action, Comedy, Martial Arts, Shounen, Super P...   TV      220      7.81   \n",
       "\n",
       "   members  \n",
       "0   683297  \n",
       "1   683297  \n",
       "2   683297  \n",
       "3   683297  \n",
       "4   683297  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducin the df using a sample to test the model faster to see if it works.\n",
    "size = 100000\n",
    "df = df.groupby(\"rating_x\", group_keys=False).apply(lambda x: x.sample(int(np.rint(size*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating_x</th>\n",
       "      <th>name</th>\n",
       "      <th>genre</th>\n",
       "      <th>type</th>\n",
       "      <th>episodes</th>\n",
       "      <th>rating_y</th>\n",
       "      <th>members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27997</td>\n",
       "      <td>194</td>\n",
       "      <td>7</td>\n",
       "      <td>Macross Zero</td>\n",
       "      <td>Adventure, Mecha, Military, Sci-Fi, Shounen</td>\n",
       "      <td>OVA</td>\n",
       "      <td>5</td>\n",
       "      <td>7.69</td>\n",
       "      <td>23568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16880</td>\n",
       "      <td>4938</td>\n",
       "      <td>8</td>\n",
       "      <td>Tsubasa: Shunraiki</td>\n",
       "      <td>Action, Adventure, Drama, Fantasy, Magic, Myst...</td>\n",
       "      <td>OVA</td>\n",
       "      <td>2</td>\n",
       "      <td>8.23</td>\n",
       "      <td>40420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39151</td>\n",
       "      <td>9989</td>\n",
       "      <td>7</td>\n",
       "      <td>Ano Hi Mita Hana no Namae wo Bokutachi wa Mada...</td>\n",
       "      <td>Drama, Slice of Life, Supernatural</td>\n",
       "      <td>TV</td>\n",
       "      <td>11</td>\n",
       "      <td>8.62</td>\n",
       "      <td>463835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  anime_id  rating_x  \\\n",
       "0    27997       194         7   \n",
       "1    16880      4938         8   \n",
       "2    39151      9989         7   \n",
       "\n",
       "                                                name  \\\n",
       "0                                       Macross Zero   \n",
       "1                                 Tsubasa: Shunraiki   \n",
       "2  Ano Hi Mita Hana no Namae wo Bokutachi wa Mada...   \n",
       "\n",
       "                                               genre type episodes  rating_y  \\\n",
       "0        Adventure, Mecha, Military, Sci-Fi, Shounen  OVA        5      7.69   \n",
       "1  Action, Adventure, Drama, Fantasy, Magic, Myst...  OVA        2      8.23   \n",
       "2                 Drama, Slice of Life, Supernatural   TV       11      8.62   \n",
       "\n",
       "   members  \n",
       "0    23568  \n",
       "1    40420  \n",
       "2   463835  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix from the ratings dataframe\n",
    "matrix = df.pivot_table(index='user_id', columns='name', values='rating_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[39m# Impute missing values with the mean rating for each anime\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m matrix \u001b[39m=\u001b[39m matrix\u001b[39m.\u001b[39;49mfillna(matrix\u001b[39m.\u001b[39;49mmean())\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n",
      "\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n",
      "\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n",
      "\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n",
      "\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n",
      "\u001b[0;32m    330\u001b[0m     )\n",
      "\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5632\u001b[0m, in \u001b[0;36mDataFrame.fillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n",
      "\u001b[0;32m   5621\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;32m   5622\u001b[0m \u001b[39m@doc\u001b[39m(NDFrame\u001b[39m.\u001b[39mfillna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_shared_doc_kwargs)\n",
      "\u001b[0;32m   5623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfillna\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   5630\u001b[0m     downcast: \u001b[39mdict\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n",
      "\u001b[0;32m   5631\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m-> 5632\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfillna(\n",
      "\u001b[0;32m   5633\u001b[0m         value\u001b[39m=\u001b[39;49mvalue,\n",
      "\u001b[0;32m   5634\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n",
      "\u001b[0;32m   5635\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n",
      "\u001b[0;32m   5636\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n",
      "\u001b[0;32m   5637\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit,\n",
      "\u001b[0;32m   5638\u001b[0m         downcast\u001b[39m=\u001b[39;49mdowncast,\n",
      "\u001b[0;32m   5639\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6871\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n",
      "\u001b[0;32m   6868\u001b[0m res_k \u001b[39m=\u001b[39m result[k]\u001b[39m.\u001b[39mfillna(v, limit\u001b[39m=\u001b[39mlimit, downcast\u001b[39m=\u001b[39mdowncast_k)\n",
      "\u001b[0;32m   6870\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n",
      "\u001b[1;32m-> 6871\u001b[0m     result[k] \u001b[39m=\u001b[39m res_k\n",
      "\u001b[0;32m   6872\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   6873\u001b[0m     \u001b[39m# We can write into our existing column(s) iff dtype\u001b[39;00m\n",
      "\u001b[0;32m   6874\u001b[0m     \u001b[39m#  was preserved.\u001b[39;00m\n",
      "\u001b[0;32m   6875\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res_k, ABCSeries):\n",
      "\u001b[0;32m   6876\u001b[0m         \u001b[39m# i.e. 'k' only shows up once in self.columns\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n",
      "\u001b[0;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n",
      "\u001b[0;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n",
      "\u001b[1;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4185\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n",
      "\u001b[0;32m   4182\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(existing_piece, DataFrame):\n",
      "\u001b[0;32m   4183\u001b[0m             value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(value, (\u001b[39mlen\u001b[39m(existing_piece\u001b[39m.\u001b[39mcolumns), \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mT\n",
      "\u001b[1;32m-> 4185\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item_mgr(key, value)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4144\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value)\u001b[0m\n",
      "\u001b[0;32m   4142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39minsert(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis), key, value)\n",
      "\u001b[0;32m   4143\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 4144\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iset_item_mgr(loc, value)\n",
      "\u001b[0;32m   4146\u001b[0m \u001b[39m# check if we are modifying a copy\u001b[39;00m\n",
      "\u001b[0;32m   4147\u001b[0m \u001b[39m# try to set first as we want an invalid\u001b[39;00m\n",
      "\u001b[0;32m   4148\u001b[0m \u001b[39m# value exception to occur first\u001b[39;00m\n",
      "\u001b[0;32m   4149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4134\u001b[0m, in \u001b[0;36mDataFrame._iset_item_mgr\u001b[1;34m(self, loc, value, inplace)\u001b[0m\n",
      "\u001b[0;32m   4130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iset_item_mgr\u001b[39m(\n",
      "\u001b[0;32m   4131\u001b[0m     \u001b[39mself\u001b[39m, loc: \u001b[39mint\u001b[39m \u001b[39m|\u001b[39m \u001b[39mslice\u001b[39m \u001b[39m|\u001b[39m np\u001b[39m.\u001b[39mndarray, value, inplace: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;32m   4132\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m   4133\u001b[0m     \u001b[39m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[39;00m\n",
      "\u001b[1;32m-> 4134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49miset(loc, value, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "\u001b[0;32m   4135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1270\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace)\u001b[0m\n",
      "\u001b[0;32m   1268\u001b[0m     removed_blknos\u001b[39m.\u001b[39mappend(blkno_l)\n",
      "\u001b[0;32m   1269\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1270\u001b[0m     nb \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mdelete(blk_locs)\n",
      "\u001b[0;32m   1271\u001b[0m     blocks_tup \u001b[39m=\u001b[39m (\n",
      "\u001b[0;32m   1272\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[:blkno_l] \u001b[39m+\u001b[39m (nb,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[blkno_l \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m :]\n",
      "\u001b[0;32m   1273\u001b[0m     )\n",
      "\u001b[0;32m   1274\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m blocks_tup\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1921\u001b[0m, in \u001b[0;36mNumpyBlock.delete\u001b[1;34m(self, loc)\u001b[0m\n",
      "\u001b[0;32m   1920\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdelete\u001b[39m(\u001b[39mself\u001b[39m, loc) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Block:\n",
      "\u001b[1;32m-> 1921\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdelete(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues, loc, \u001b[39m0\u001b[39;49m)\n",
      "\u001b[0;32m   1922\u001b[0m     mgr_locs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr_locs\u001b[39m.\u001b[39mdelete(loc)\n",
      "\u001b[0;32m   1923\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)(values, placement\u001b[39m=\u001b[39mmgr_locs, ndim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim)\n",
      "\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdelete\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Impute missing values with the mean rating for each anime\n",
    "matrix = matrix.fillna(matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved matrix\n",
    "matrix = joblib.load(processed_data + \"/\" + \"tests_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.91 GiB for an array with shape (11196, 58812) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Split the data into training and test sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(matrix, matrix, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2585\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m-> 2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[0;32m   2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2587\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m   2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:354\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    349\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSpecifying the columns using strings is only supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpandas DataFrames\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m     )\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:196\u001b[0m, in \u001b[0;36m_pandas_indexing\u001b[1;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    191\u001b[0m     key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m key_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39misscalar(key)):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# copy that will not raise SettingWithCopyWarning\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m X\u001b[39m.\u001b[39;49mtake(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     indexer \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39miloc \u001b[39mif\u001b[39;00m key_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mloc\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3871\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3862\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   3863\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis_copy is deprecated and will be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m'\u001b[39m\u001b[39m always returns a copy, so there is no need to specify this.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3865\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m   3866\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   3867\u001b[0m     )\n\u001b[0;32m   3869\u001b[0m nv\u001b[39m.\u001b[39mvalidate_take((), kwargs)\n\u001b[1;32m-> 3871\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices, axis)\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3879\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3880\u001b[0m \u001b[39mInternal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[0;32m   3881\u001b[0m \n\u001b[0;32m   3882\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m-> 3886\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   3887\u001b[0m     indices,\n\u001b[0;32m   3888\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   3889\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   3890\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[0;32m   3891\u001b[0m )\n\u001b[0;32m   3892\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:980\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    977\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[0;32m    979\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 980\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[0;32m    981\u001b[0m     new_axis\u001b[39m=\u001b[39;49mnew_labels,\n\u001b[0;32m    982\u001b[0m     indexer\u001b[39m=\u001b[39;49mindexer,\n\u001b[0;32m    983\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    984\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    985\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    986\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:753\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    751\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 753\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m    754\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    755\u001b[0m             indexer,\n\u001b[0;32m    756\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    757\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    758\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[0;32m    759\u001b[0m             ),\n\u001b[0;32m    760\u001b[0m         )\n\u001b[0;32m    761\u001b[0m         \u001b[39mfor\u001b[39;49;00m blk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks\n\u001b[0;32m    762\u001b[0m     ]\n\u001b[0;32m    763\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:754\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    751\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 754\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    755\u001b[0m             indexer,\n\u001b[0;32m    756\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    757\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    758\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[0;32m    759\u001b[0m             ),\n\u001b[0;32m    760\u001b[0m         )\n\u001b[0;32m    761\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[0;32m    762\u001b[0m     ]\n\u001b[0;32m    763\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:880\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m    877\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m    881\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[0;32m    882\u001b[0m )\n\u001b[0;32m    884\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[39m#  this assertion\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m new_mgr_locs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\christiandda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:158\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    160\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    161\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.91 GiB for an array with shape (11196, 58812) and data type float64"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix, matrix, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for model training and evaluation\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate seven different models\n",
    "models = [TruncatedSVD(), PCA(), NearestNeighbors(), Lasso(), Ridge(), RandomForestRegressor()]\n",
    "model_names = ['Truncated SVD', 'PCA', 'Nearest Neighbors', 'Lasso', 'Ridge', 'Random Forest Regressor']\n",
    "best_mse = np.inf\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "for i, model in enumerate(models):\n",
    "    mse = train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "    print(\"Model:\", model_names[i], \"MSE:\", mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        best_model_name = model_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model and predict N number of animes for each user\n",
    "N = 10\n",
    "best_model.fit(matrix, matrix)\n",
    "user_predictions = best_model.predict(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of anime names and predicted ratings\n",
    "anime_names = matrix.columns\n",
    "prediction_df = pd.Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "sounds_folder = (main_folder + \"\\sounds\")\n",
    "saved_models = (main_folder + \"\\saved_models\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "user_based_unsupervised_data = (data_folder + \"\\processed\\_user_based_unsupervised\")\n",
    "content_based_unsupervised_data = (data_folder + \"\\processed\\content_based_unsupervised\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime ratings dataset into a pandas dataframe\n",
    "df = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "# Merge the ratings dataframe with the anime names dataframe\n",
    "df = pd.merge(df, pd.read_csv(raw_data + \"/\" + \"anime.csv\"), on='anime_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix from the ratings dataframe\n",
    "matrix = df.pivot_table(index='user_id', columns='anime_name', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved matrix\n",
    "matrix = joblib.load(processed_data + \"/\" + \"tests_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the mean rating for each anime\n",
    "matrix = matrix.fillna(matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix, matrix, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for model training and evaluation\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate different models\n",
    "models = [KNeighborsClassifier(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "model_names = ['KNeighbors Classifier', 'SVM Classifier', 'Decision Tree Classifier', 'Random Forest Classifier']\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "for i, model in enumerate(models):\n",
    "    accuracy = train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "    print(\"Model:\", model_names[i], \"Accuracy:\", accuracy)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "        best_model_name = model_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model and predict N number of animes for each user\n",
    "N = \n",
    "input_anime = input(\"Enter an anime name:\")\n",
    "anime_index = np.where(matrix.columns == input_anime)[0][0]\n",
    "user_ratings = matrix.iloc[:, anime_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the top N similar animes for the input anime\n",
    "best_model.fit(matrix, user_ratings)\n",
    "anime_similarities = best_model.predict(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions in a dataframe sorted from higher to lower\n",
    "prediction_df = df.iloc[most_similar]\n",
    "prediction_df = prediction_df.sort_values(by=\"rating\", ascending=False)\n",
    "prediction_df.to_csv(\"anime_recommendations.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anime recommendation system using an SVC model with :\n",
    "- hyperparameter tuning\n",
    "- cross-validation\n",
    "- evaluation\n",
    "- StandardScaler\n",
    "- StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "sounds_folder = (main_folder + \"\\sounds\")\n",
    "saved_models = (main_folder + \"\\saved_models\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "user_based_unsupervised_data = (data_folder + \"\\processed\\_user_based_unsupervised\")\n",
    "content_based_unsupervised_data = (data_folder + \"\\processed\\content_based_unsupervised\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime dataset\n",
    "df_name = pd.read_csv(raw_data + \"/\" + \"anime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for modeling\n",
    "X = df.drop(columns=[\"anime_name\", \"rating\"])\n",
    "y = df[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [1, 5, 10, 50], 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Create a SVC model\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=skf, scoring='accuracy')\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best hyperparameters\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVC model using the best hyperparameters\n",
    "svc = SVC(C=best_params[\"C\"], kernel=best_params[\"kernel\"])\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings for a specific anime\n",
    "input_anime = \"Naruto\"\n",
    "input_index = df[df[\"anime_name\"] == input_anime].index[0]\n",
    "input_features = X[input_index, :].reshape(1, -1)\n",
    "input_rating = svc.predict(input_features)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the N most similar animes\n",
    "N = 10\n",
    "predictions = svc.predict(X)\n",
    "similarity_scores = np.abs(predictions - input_rating)\n",
    "most_similar = np.argsort(similarity_scores)[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions in a dataframe sorted from higher to lower\n",
    "prediction_df = df.iloc[most_similar]\n",
    "prediction_df = prediction_df.sort_values(by=\"rating\", ascending=False)\n",
    "prediction_df.to_csv(\"anime_recommendations.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anime recommendation system using a regression model with:\n",
    "- hyperparameter tuning\n",
    "- cross-validation\n",
    "- evaluation\n",
    "- StandardScaler\n",
    "- StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "sounds_folder = (main_folder + \"\\sounds\")\n",
    "saved_models = (main_folder + \"\\saved_models\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "user_based_unsupervised_data = (data_folder + \"\\processed\\_user_based_unsupervised\")\n",
    "content_based_unsupervised_data = (data_folder + \"\\processed\\content_based_unsupervised\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime dataset\n",
    "df = pd.read_csv(raw_data + \"/\" + \"anime.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "X = df.drop(\"rating\", axis=1)\n",
    "y = df[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline for scaling and regression\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                 (\"reg\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid search with cross-validation\n",
    "grid = GridSearchCV(pipe, param_grid, cv=skf, n_jobs=-1, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyperparameters and model\n",
    "best_params = grid.best_params_\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings for the input anime name\n",
    "input_anime = \"One Punch Man\"\n",
    "input_df = df[df[\"name\"] == input_anime].drop(\"rating\", axis=1)\n",
    "predictions = best_model.predict(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the N number of similar animes\n",
    "N = 10\n",
    "top_N = np.argsort(predictions)[-N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the top N similar animes\n",
    "top_N_animes = df.iloc[top_N][\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the predictions and names of the top N similar animes\n",
    "prediction_df = pd.DataFrame({\"anime_name\": top_N_animes,\n",
    "                              \"prediction\": predictions[top_N]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe from higher to lower based on the predictions\n",
    "prediction_df.sort_values(\"prediction\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the dataframe\n",
    "prediction_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe\n",
    "print(prediction_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 17"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anime recommendation system using XGBoost with:\n",
    "- hyperparameter tuning\n",
    "- cross-validation\n",
    "- evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "sounds_folder = (main_folder + \"\\sounds\")\n",
    "saved_models = (main_folder + \"\\saved_models\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "user_based_unsupervised_data = (data_folder + \"\\processed\\_user_based_unsupervised\")\n",
    "content_based_unsupervised_data = (data_folder + \"\\processed\\content_based_unsupervised\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime data\n",
    "anime_data = pd.read_csv(raw_data + \"/\" + \"anime.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "anime_data.drop([\"anime_id\", \"name\", \"genres\"], axis=1, inplace=True)\n",
    "\n",
    "# Get the target variable\n",
    "target = anime_data[\"rating\"]\n",
    "anime_data.drop([\"rating\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(anime_data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n",
    "              \"n_estimators\": [100, 200, 300],\n",
    "              \"max_depth\": [3, 5, 7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "xgb_grid = GridSearchCV(xgb_model, parameters, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "xgb_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_params = xgb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with the best hyperparameters\n",
    "xgb_final = xgb.XGBRegressor(learning_rate=best_params[\"learning_rate\"],\n",
    "                             n_estimators=best_params[\"n_estimators\"],\n",
    "                             max_depth=best_params[\"max_depth\"])\n",
    "xgb_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "mse = mean_squared_error(y_test, xgb_final.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings for a specific anime\n",
    "input_anime = \"One Piece\"\n",
    "input_anime_index = anime_data[anime_data[\"name\"] == input_anime].index[0]\n",
    "input_anime_features = anime_data.iloc[input_anime_index].values.reshape(1, -1)\n",
    "input_anime_rating = xgb_final.predict(input_anime_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings for N number of animes\n",
    "N = 10\n",
    "anime_ratings = xgb_final.predict(anime_data)\n",
    "top_N_indexes = np.argsort(anime_ratings)[::-1][:N]\n",
    "top_N_animes = anime_data.iloc[top_N_indexes][\"name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions in a dataframe sorted from higher to lower\n",
    "results = pd.DataFrame({\"anime\": top_N_animes, \"rating\": anime_ratings[top_N_indexes]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe from higher to lower based on the predictions\n",
    "prediction_df.sort_values(\"prediction\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the dataframe\n",
    "prediction_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe\n",
    "print(prediction_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 22"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix factorization-based recommendation system, using the Singular Value Decomposition (SVD) algorithm, and evaluates its performance using precision, recall, and F1-score metrics-"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the Surprise library, which provides an easy-to-use implementation of matrix factorization-based recommendation algorithms. The user-item matrix is loaded into the Surprise library's Dataset object, which can handle missing values and sparse matrices. The data is then split into training and test sets, and the SVD model is trained on the training set. The performance of the model is evaluated on the test set by computing precision, recall, and F1-score metrics, which are commonly used metrics for evaluating recommendation systems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the Surprise library's SVD model to make predictions for each user and each anime in the dataset. The predictions are stored in a dictionary, with the user ID as the key and a list of tuples of anime ID and predicted rating as the value. The dictionary is then converted to a Pandas DataFrame and saved to a CSV file. The code defines the number of anime recommendations to predict for each user as N = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Preparing folder variables\n",
    "os.chdir(os.path.dirname(sys.path[0])) # This command makes the notebook the main path and can work in cascade.\n",
    "main_folder = sys.path[0]\n",
    "data_folder = (main_folder + \"\\data\")\n",
    "saved_models_folder = (data_folder + \"\\saved_models\")\n",
    "sounds_folder = (main_folder + \"\\sounds\")\n",
    "saved_models = (main_folder + \"\\saved_models\")\n",
    "processed_data = (data_folder + \"\\processed\")\n",
    "raw_data = (data_folder + \"\\_raw\")\n",
    "user_based_unsupervised_data = (data_folder + \"\\processed\\_user_based_unsupervised\")\n",
    "content_based_unsupervised_data = (data_folder + \"\\processed\\content_based_unsupervised\")\n",
    "content_based_supervised_data = (data_folder + \"\\processed\\content_based_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anime ratings dataset into a pandas DataFrame\n",
    "df = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "# Create a user-item matrix from the ratings dataframe\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'anime_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVD model\n",
    "model = SVD()\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings for the test set\n",
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of anime recommendations to predict for each user\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, and F1-score metrics\n",
    "y_true = [pred.r_ui for pred in predictions]\n",
    "y_pred = [pred.est for pred in predictions]\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for each user\n",
    "user_predictions = {}\n",
    "for user_id in df['user_id'].unique():\n",
    "    anime_ratings = []\n",
    "    for anime_id in df['anime_id'].unique():\n",
    "        pred = model.predict(user_id, anime_id)\n",
    "        anime_ratings.append((anime_id, pred[3]))\n",
    "    anime_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "    user_predictions[user_id] = anime_ratings[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions to a DataFrame and write to a CSV file\n",
    "pred_df = pd.DataFrame.from_dict(user_predictions, orient='index')\n",
    "pred_df.to_csv('user_predictions.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations for a specific user by user ID using the SVD (Singular Value Decomposition) model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD model from the surprise library is used to make predictions about the ratings for the test set. The function get_user_predictions is used to get the predictions for a specific user ID by filtering the ratings for that user from the trainset and passing them to the test method of the SVD model. The predictions are sorted from highest to lowest, and N animes are recommended to the user. The recommendations are merged with the anime names dataframe and saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ratings dataset into a pandas dataframe\n",
    "df = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe into a Surprise dataset format\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'anime_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to make predictions for the test set\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the predictions\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions for a specific user ID\n",
    "def get_user_predictions(user_id, algo, trainset):\n",
    "    user_inner_id = algo.trainset.to_inner_uid(user_id)\n",
    "    user_ratings = algo.trainset.ur[user_inner_id]\n",
    "    user_predictions = algo.test(user_ratings)\n",
    "    return user_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 123\n",
    "user_predictions = get_user_predictions(user_id, algo, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the predictions from highest to lowest\n",
    "user_predictions.sort(key=lambda x: x.est, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend N animes for the user\n",
    "N = 10\n",
    "user_recommendations = [prediction.iid for prediction in user_predictions[:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the recommendations with the anime names dataframe\n",
    "recommendations_df = pd.merge(df[['anime_id', 'anime_name']], \n",
    "                              pd.DataFrame({'anime_id': user_recommendations}), \n",
    "                              on='anime_id', \n",
    "                              how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the recommendations to a csv file\n",
    "recommendations_df.to_csv('user_{}_recommendations.csv'.format(user_id), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise library to recommend animes to a user using the Singular Value Decomposition (SVD) model, and predicts the rating that the user would give to each recommended anime:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Load the ratings dataset into a pandas dataframe\n",
    "df = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "# Create a reader object\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "\n",
    "# Load the data into a dataset using the reader\n",
    "data = Dataset.load_from_df(df[['user_id', 'anime_id', 'rating']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train the SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "\n",
    "# Load the ratings dataset into a pandas dataframe\n",
    "df = pd.read_csv(raw_data + \"/\" + \"rating.csv.zip\")\n",
    "\n",
    "# Get the anime names dataframe\n",
    "anime_names = pd.read_csv(raw_data + \"/\" + \"anime.csv.zip\")\n",
    "\n",
    "# Function to recommend animes to a user\n",
    "def recommend_animes(user_id, N):\n",
    "    # Get the user's rated animes\n",
    "    user_ratings = df[df['user_id'] == user_id]\n",
    "    \n",
    "    # Get the user's anime IDs\n",
    "    user_anime_ids = user_ratings['anime_id'].tolist()\n",
    "    \n",
    "    # Get the anime names for the user's rated animes\n",
    "    user_anime_names = anime_names[anime_names['anime_id'].isin(user_anime_ids)].anime_name.tolist()\n",
    "    \n",
    "    # Get the predicted ratings for the animes that the user has not rated\n",
    "    predictions = []\n",
    "    for anime_id in anime_names['anime_id']:\n",
    "        if anime_id not in user_anime_ids:\n",
    "            prediction = algo.predict(user_id, anime_id)\n",
    "            predictions.append((prediction.est, anime_id))\n",
    "    \n",
    "    # Sort the predictions from highest to lowest\n",
    "    predictions = sorted(predictions, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Get the anime names for the recommended animes\n",
    "    recommendations = [anime_names[anime_names['anime_id'] == anime_id].anime_name.values[0] for _, anime_id in predictions[:N]]\n",
    "    \n",
    "    # Get the predicted ratings for the recommended animes\n",
    "    prediction_ratings = [prediction for prediction, _ in predictions[:N]]\n",
    "    \n",
    "    # Return the recommendations and prediction ratings\n",
    "    return recommendations, prediction_ratings\n",
    "\n",
    "# Recommend N animes to a user\n",
    "user_id = 12345\n",
    "N = 10\n",
    "recommendations, prediction_ratings = recommend_animes(user_id, N)\n",
    "\n",
    "# Print the recommendations and prediction ratings\n",
    "print(\"Recommended Animes:\")\n",
    "for i in range(N):\n",
    "    print(f\"{i+1}. {recommendations[i]} ({prediction_ratings[i]:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cff4678dccc161ee1c6f25be5e3677ea130d7193750bbc4b79c480bbe89cc2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
